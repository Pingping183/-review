# cost function
loss function 是cost function在单样本上的特例
cost function 是求和



## mean square loss


# 欧几里得数据类型
欧几里得数据类型是在欧式空间中表示的数据，具有 $\textbf{平移不变性和排列整齐}$ 的特点，常见于图像、文本等规则结构的数据中。
图是一种非欧几里得数据类型，它存在于三维空间，不像其他数据类型如图像、文本和音频。由于图不具备平移不变性，卷积神经网络等传统方法无法有效提取其结构信息，因此衍生出了处理这类数据的网络，即图神经网络。（图论中的图）

# 详解最大似然估计（MLE）、最大后验概率估计（MAP），以及贝叶斯公式的理解

这两种是常见的参数估计的方法
MAP和MLE是统计的内容（是知道数据去推模型，参数）

 $P(X|\theta)$ 
1. 若X已知， $\theta$ 未知，则是似然函数（根据现有的X这个样本，选一个 $\theta$ ，让这样的X出现的概率最大）--不足是这样的X不一定反映了真实的 $\theta$ ，因此需要知道 $P(\theta) \theta$ 的先验概率，乘以 $P(\theta)$ ，使这个值最大，才是真实的 $\theta$ 的值。也就是最大化 $P(X|\theta)P(\theta)$ .这个就是MAP。（需要预先知道 $P(\theta)$ 的信息)

2. 若X未知，$\theta$ 已知，则是概率函数。

参考文件

https://blog.csdn.net/u011508640/article/details/72815981

# pc算法因果网络
只是构建因果图，有图有箭头，没有数值。
通过相关性可以弄出图，但是没有箭头，通过这个算法可以弄出箭头。

## PC算法在因果网络中的应用
PC算法是因果发现中常用的一种方法，基于概率图模型的理论，通过分析变量之间的条件独立性来推断因果关系。
PC方法包括两个基本步骤：图的构建和图的修剪。
图的构建阶段：基于数据中的变量之间的相关性构建一个初始的无向图。然后使用条件独立性测试来检验变量之间的独立性关系，从而将图中的边进行有向化。
图的修剪阶段：从初始图中去除不可能的因果关系，以得到一个更精确的因果图。这个阶段使用启发式算法，如PC算法、FSPC算法等，来根据变量之间的条件独立性关系判断是否可以去除某些边。

## PC算法的优点与局限性
优点：PC算法可以在实际应用中进行因果推断，而不需要进行实验，这使得它在许多领域具有优势。适用于高维稀疏因果图。
局限性：PC算法对数据的要求比较高，需要大量的样本数据来进行准确的推断。此外，面对复杂的因果关系时，PC算法可能会产生错误的推断结果。

## 寻找DAG (direct acyclic graph)的等价类
==概念==：
如果能从G图中的d-seperation推断出分布是条件独立的，那么就说这个分布对G是faithful的。反之亦然（如果分布对G是faithful的，那么就能从G图中的d-seperation推断出分布是条件独立的）
==在给定V中其他某些变量向量的情况下,i,j结点的向量是独立的==P相对于G是忠诚的（P 是所有结点的分布）
结点是随机向量x的分量，X是一个p维的向量，X的每个分量取值都是实数。所以每个结点的取值都是实数。



## 辛普森悖论
如果忽略了第三个变量（如性别）=，那结论会完全相反---不能忽略性别的因



# 降维

## 主成分分析（PCA）

## 多维缩放(MDS)

## 线性判别分析(LDA)

## 等度量映射(Isomap)

## t-sne
t-distributed Stochastic Neighbor Embedding，翻译为 t分布-随机邻近嵌入
详见：C:\Users\m00842394\Desktop\vscodelearning\t-sne原理.md
这里有代码：https://www.datacamp.com/tutorial/introduction-t-sne
参考材料：Visualizing Data using t-SNE，2008年发表在Journal of Machine Learning Research，大神Hinton的文章：http://www.jmlr.org/papers/v9/vandermaaten08a.html
参考资料：https://cloud.tencent.com/developer/article/1549992

t-SNE 让您直观地了解数据在高维空间中的排列方式。它通常用于将复杂数据集可视化为二维和三维空间，让我们能够更好地了解数据中潜在的模式和关系。

### 步骤
t-SNE 模拟一个点在高维和低维中被选为另一个点的邻居。它首先使用`高斯核`计算高维空间中所有数据点之间的成对相似性。相距较远的点被选中的概率低于相距较近的点。   
然后，该算法尝试将高维数据点 映射 到低维空间，同时保留成对的相似性。   
它是通过最小化原始高维和低维概率分布之间的`Kullback-Leibler (KL) 散度`来实现的。该算法使用`梯度下降`来最小化散度。低维嵌入被优化到稳定状态。  

对于 t-SNE 算法来说，`困惑度`是一个非常重要的超参数，它控制着每个点在降维过程中考虑的有效邻居数量。 


SNE是通过`仿射`(affinitie)变换将数据点映射到概率分布上，主要包括两个步骤：



>t-SNE将数据点之间的相似度转化为条件概率，原始空间中数据点的相似度由高斯联合分布表示，嵌入空间中数据点的相似度由学生t分布 表示。

>在文中提到的论文中，主要讨论降维出现的拥挤问题，解决的方法也很巧妙，一旦理解它后就明白为什么叫t-分布随机近邻嵌入。

如果想象在一个三维的球里面有均匀分布的点，不难想象，如果把这些点投影到一个二维的圆上一定会有很多点是重合的。

所以，为了在二维的圆上想尽可能表达出三维里的点的信息，大神Hinton采取的方法：

把由于投影所重合的点用不同的距离（差别很小）表示。

这样就会占用原来在那些距离上的点，原来那些点会被赶到更远一点的地方。

t分布是长尾的，意味着距离更远的点依然能给出和高斯分布下距离小的点相同的概率值。

从而达到高维空间和低维空间对应的点概率相同的目的。

通过原始空间和嵌入空间的联合概率分布的KL散度（用于评估两个分布的相似度的指标，经常用于评估机器学习模型的好坏）来评估嵌入效果的好坏。

也就是，将有关KL散度的函数作为损失函数（loss function），通过梯度下降算法最小化损失函数，最终获得收敛结果。

### 对比pca
t-SNE 与 PCA
t-SNE 和 PCA 都是降维技术，它们具有不同的机制，并且适用于不同类型的数据。

PCA（主成分分析）是一种线性技术，最适合用于具有线性结构的数据。它试图通过投影到较低维度、最小化方差和保留较大的成对距离来识别数据中的潜在主成分。阅读我们的主成分分析 (PCA)教程，通过 R 示例了解算法的内部工作原理。 

但是，t-SNE 是一种非线性技术，专注于保持低维空间中数据点之间的成对相似性。t-SNE 关注的是保持较小的成对距离，而 PCA 则专注于维持较大的成对距离以最大化方差。

总结一下，PCA保留了数据中的方差，而t-SNE保留了低维空间中数据点之间的关系，因此它是一种很好的复杂高维数据可视化算法。 
### 应用
该算法在论文中非常常见，主要用于高维数据的降维和可视化。

除了可视化复杂的多维数据之外，t-SNE 还有其他用途，主要用于医学领域。

t-SNE变换后，如果在低维空间中具有可分性，则数据是可分的；如果在低维空间中不可分，则可能是因为数据集本身不可分，或者数据集中的数据不适合投影到低维空间。



聚类和分类：在低维空间中将相似的数据点聚类在一起。它也可以用于分类和查找数据中的模式。 
异常检测：识别数据中的异常值和异常。 


# 梯度下降算法
参考资料：https://www.cnblogs.com/pinard/p/5970503.html

无约束优化算法的一种，其他还有最小二乘法，此外还有牛顿法和拟牛顿法。
梯度下降法就是对参数求导，参数(t) = 参数（t-1）-step * d参数
d参数 = 函数对参数求微分
找到最优的参数，判断方法：

因为不是单纯的递增或者递减，某参数的变动和过去的该参数，过去的其他参数有关，每次你这个方向动一点（可能别的参数不变的话，单看你的变化对整体是增加），其他参数方向动一点，才能让整体下降最快。

## 和最小二乘、牛顿法的区别：
梯度下降法和最小二乘法相比，梯度下降法需要选择步长，而最小二乘法不需要。梯度下降法是迭代求解，最小二乘法是计算解析解。如果样本量不算很大，且存在解析解，最小二乘法比起梯度下降法要有优势，计算速度很快。但是如果样本量很大，用最小二乘法由于需要求一个超级大的逆矩阵，这时就很难或者很慢才能求解解析解了，使用迭代的梯度下降法比较有优势。

梯度下降法和牛顿法/拟牛顿法相比，两者都是迭代求解，不过梯度下降法是梯度求解，而牛顿法/拟牛顿法是用二阶的海森矩阵的逆矩阵或伪逆矩阵求解。相对而言，使用牛顿法/拟牛顿法收敛更快。但是每次迭代的时间比梯度下降法长。

## 梯度下降的调优
如果损失函数不是凸函数，会受到初始值、步长的影响，只能得到局部最优解，所以初始值、步长可以调整
迭代速度可以调优

### AdaGrad的梯度下降算法
学习曲率

### 随机梯度下降
stochastic gradient descent SGD
每次优化的样本随机取几个，防止样本量太大导致计算太慢。

# 牛顿法
优化算法之一。牛顿法其实很简单，就是通过切线与x轴的交点不断更新切线的位置，直到达到曲线与x轴的交点得到方程解。

在实际应用中我们因为常常要求解凸优化问题，也就是要求解函数一阶导数为0的位置，而牛顿法恰好可以给这种问题提供解决方法。

实际应用中牛顿法首先选择一个点作为起始点，并进行一次二阶泰勒展开得到导数为0的点进行一个更新，直到达到要求，这时牛顿法也就成了二阶求解问题，比一阶方法更快。我们常常看到的x通常为一个多维向量，这也就引出了Hessian矩阵的概念（就是x的二阶导数矩阵）。

缺点：牛顿法是定长迭代，没有步长因子，所以不能保证函数值稳定的下降，严重时甚至会失败。还有就是牛顿法要求函数一定是二阶可导的。而且计算Hessian矩阵的逆复杂度很大。
为了求损失函数的最小值，把损失函数用泰勒展开到2阶，然后求导，令=0，求出x的解，带入。此处认为x是k+1时刻的x, $x_0$ 是k时刻的x.从 $x_0$ 挪一步变成 $x$ ，继续迭代，直到f(x)的变化小于规定值。
把f(x)用泰勒展开到二阶：

$$
f(x) = f(x_0) + f'(x_0)(x-x_0)+\frac{1}{2}f''(x_0)(x-x_0)^2
$$

f(x)对x求导，令其为0，得到x的解。

$$
f'(x) = f'(x_0)+f''(x_0)(x-x_0) = 0 \\
x = x_0-\frac{f'(x_0)}{f''(x_0)}
$$

也即，

$$
x_{k+1} = x_k - \frac{f'(x_0)}{f''(x_0)}
$$

直到 $f(x_{k+1}-f(x_k))$ 小于 $\epsilon$ 


[![牛顿法](image-46.png)](https://blog.csdn.net/google19890102/article/details/41087931)
## 拟牛顿法
不用二阶偏导而是构造出Hessian矩阵的近似正定对称矩阵的方法称为拟牛顿法。拟牛顿法的思路就是用一个特别的表达形式来模拟Hessian矩阵或者是他的逆使得表达式满足拟牛顿条件。主要有DFP法（逼近Hession的逆）、BFGS（直接逼近Hession矩阵）、 L-BFGS（可以减少BFGS所需的存储空间）。

# 交叉验证
训练集和测试集的划分方法会影响模型以及模型的结果
而且不能充分利用数据，（测试集的数据没法用来训练）

## 1.模型的评价标准
如果是回归问题是对每次得到的MSE取平均。（如果是分类问题，就是错误分类个数的平均）

## 2.用途
交叉验证的这个手段一般可用于模型的“选择”，通过交叉验证后学习器的综合表现来选择不同的模型和超参数。
选了模型之后调参，使得MSE最小。

## 3.LOOCV
每次只取一个数据作为测试数据，剩下的作为训练集，对每次得到的MSE取平均。（或者错误分类个数的平均如果是分类问题）
不受划分的影响，模型的Bias是最小的，但是耗时大。----》
![LOOCV改善](LOOCV改善.png)
这个没懂

## 4.K折

折K次进行交叉验证，对得到的MSE 取平均
K的取值：
K越大，每次投入的训练集的数据越多，模型的Bias越小。但是K越大，又意味着每一次选取的训练集之前的相关性越大（考虑最极端的例子，当k=N，也就是在LOOCV里，每次都训练数据几乎是一样的）。而这种大相关性会导致最终的test error具有更大的Variance。（没懂偏倚和test error)

一般来说，根据经验我们一般选择k=5或10。

# 逻辑回归
逻辑回归的cost function 不用平方误差，因为平方误差是非凸的，一般用

$$
J(w,b) = \frac{1}{m}\sum^m_{i=1}L(\hat{y}^{(i)},y^{(i)}) =
-\frac{1}{m} \sum^m_{i=1}[ -(y^{(i)}log\hat{y}^{(i)} + (1-y^{(i)}) log(1-\hat{y}^{(i)}))]
$$

也是：条件对数似然，如果这样理解拟合函数 ： $\hat{y} = P(y=1|x)$
## 逻辑回归调优
梯度下降、随机梯度下降、mini 随机梯度下降降法。随机梯度下降不但速度上比原始梯度下降要快，局部最优化问题时可以一定程度上抑制局部最优解的发生。

## 逻辑斯特回归为什么要对特征进行离散化。
非线性！非线性！非线性！逻辑回归属于`广义线性模型`，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合； 离散特征的增加和减少都很容易，易于模型的快速迭代；
>'广义线性模型`


速度快！速度快！速度快！稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；
鲁棒性！鲁棒性！鲁棒性！离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；

方便交叉与特征组合：离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力；

稳定性：特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问；


简化模型：特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。

##  逻辑回归的目标函数中增大L1正则化会是什么结果。
所有的参数w都会变成0。



# 正则化
参考：https://cloud.tencent.com/developer/article/1456966

正则化是一种用来改善模型泛化能力，减少过拟合的技术。

目标函数（通常是最小化损失函数）不仅考虑了模型对训练数据的拟合程度，还可能会包含正则化项以防止模型过拟合。

常见的惩罚项有L0、L1和L2惩罚项，
其中L0惩罚项为权值向量W中不为0的分量个数，
L1惩罚项为权值向量W各分量的绝对值之和，（L1正则化也是绝对值之和）
这两个惩罚项皆可以很好地维持权值W的稀疏性。
单目标值时，L2惩罚项为权值向量W的模，（平方和开根号），叫做L2惩罚项（L2正则化是不开根号的，但是都是和平方和有关的）
多目标值时，L2惩罚项为权值矩阵W的奇异值的最大值，L2惩罚项可以很好地防止模型过拟合。

## L0正则化

既然L0可以实现稀疏，为什么不用L0，而要用L1呢？个人理解一是因为L0范数很难优化求解（NP难问题），二是L1范数是L0范数的最优凸近似，而且它比L0范数要容易优化求解。所以大家才把目光和万千宠爱转于L1范数。


## L1正则化（也称为Lasso正则化）

是在目标函数中添加一个关于模型参数绝对值的和（即参数向量的L1范数）的惩罚项。增大L1正则化的强度意味着在目标函数中增加这个惩罚项的权重，这会导致模型在训练过程中更倾向于选择参数绝对值较小的解，甚至使某些参数变为0（因此具有特征选择的效果），从而使模型变得更加简单和稀疏。

具体来说，在逻辑回归中，假设我们的目标是最小化交叉熵损失函数 L(w,b)（其中 w 是权重，b 是偏置项），那么加上L1正则化后的目标函数可以表示为：

$$
J(w,b) = L(w,b) + \lambda \sum^n_{i-1}|w_i|
$$

其中，λ 是正则化强度系数，它控制了正则化项在目标函数中的影响程度。n 是权重的数量（不包括偏置项 b，因为通常不对偏置项进行正则化）。

增大 λ 的值意味着增大L1正则化的强度，这会导致模型在训练过程中更加注重减小参数的绝对值之和，进而可能使得某些权重 $w_i$ 

​变为0，从而达到稀疏化的效果。稀疏化有助于减少模型的复杂性，避免过拟合，同时也可能使模型更加容易解释（因为很多特征对应的权重为0，意味着这些特征在预测时不起作用）。

然而，需要注意的是，正则化强度的选择是一个重要的超参数调优过程，过大的 λ 值可能导致模型欠拟合，即模型无法充分学习到数据的特征，导致在训练集和测试集上的表现都很差。因此，在实际应用中，需要通过交叉验证等方法来选择合适的 λ 值。

## L2正则化
（也称为权重衰减、岭回归（Ridge Regression）或Tikhonov正则化）是一种在机器学习和深度学习中广泛使用的正则化技术，旨在减少模型的复杂度，防止模型过拟合，并提高模型的泛化能力。L2正则化通过在损失函数中添加一个对模型权重平方的惩罚项来实现这一目标。

基本原理
在没有正则化的情况下，模型的损失函数（如均方误差、交叉熵等）通常只包含与数据拟合度相关的项。加入L2正则化后，损失函数会变为：

基本原理
在没有正则化的情况下，模型的损失函数（如均方误差、交叉熵等）通常只包含与数据拟合度相关的项。加入L2正则化后，损失函数会变为：

$$
L(w)=L _0(w) + \frac{\lambda}{2n}\sum_jw_j^2
$$

其中：
 $L_0(w)$ 是原始损失函数，w 是模型权重。

λ 是正则化强度或正则化系数，用于控制正则化项对总损失的影响程度。较大的 λ 值会导致更强的正则化效果，即更小的权重值。

n 是训练样本的数量，这个因子通常用于归一化，但在实践中，有时为了简化计算会省略。

$\sum_jw_j^2$ 是权重向量的L2范数（即权重的平方和），它惩罚了较大的权重值，倾向于使权重更加分散，避免模型过于依赖某些特征。


>岭回归和l2范数，ridge
回顾一下最小二乘法：https://www.cnblogs.com/wellp/p/9286240.html

$$
\hat{y} = X\beta \\
w^* = argmin_w \sum_{i = 1}^n(\hat{y}-y)^2
$$

假设样本格式是n，特征个数是p，那么 $y_{(n*1)}$,$x_{n*2}$,$\beta_{n*2}$ 
ps:X第二列是1， 有一个常数项
损失函数C对 $\beta$ 求导，用矩阵表示

$
C(\beta) = ( x \beta-y)'(x \beta -y) = \beta' x' x \beta -y'x\beta-  +y'y \\
$

令：

$$
\frac{\partial C}{\partial \beta} = ((x'x)'+x'x)\beta -2x'y = 0 \\
 2x'x \beta -2x'y = 0 \\
解得：\beta^* = (x'x)^{-1}x'y  \\
$$

$
remark: \frac{\partial x'Ax}{\partial x} = (A'+A)x ,   
\frac{\partial x'A}{\partial x} = \frac{\partial A'x}{\partial x} =  A
$

岭回归：
目的：在求解参数的时候，为了解的稳定性，因为不稳定的解（矩阵），会导致，虽然输入的x差不多，但是输出的y差很多
而矩阵的这种稳定性用条件数来表示，条件数太大会导致这个矩阵不稳定
于是在损失函数里面加了一个L2正则项，让这个x'x 变成 $(x'x+\lambda I)$ 矩阵一定满秩，==一定可逆。如果不可逆的话，通过这个解，样本x对y的预测会很不稳定，而这个规则项的引入则可以改善条件数。
![岭回归](image-47.png)
于是损失函数变成 $C(\beta) = ( x \beta-y)'(x \beta -y) + \beta'\beta$ 
求导得到解：$\beta^* = (x'x+\lambda I)^{-1} x'y$ 

### 作用
防止过拟合：通过限制权重的大小，L2正则化可以减少模型的复杂度，防止模型在训练数据上学习得过于精细，从而避免在未见过的数据上表现不佳（即过拟合）。
提高泛化能力：较小的权重值意味着模型在输入数据上的变化不会过于敏感，从而提高了模型在不同数据上的表现能力，即泛化能力。
数学上的稳定性：L2正则化使得损失函数成为强凸函数，这有助于在优化过程中找到全局最优解，并减少优化算法的波动。

## L1和L2的区别
L1的功能是使权重稀疏，而L2的功能是使权重平滑。

### L1正则为什么可以得到稀疏解？L2得到权重平滑的解？
参考材料：
https://cloud.tencent.com/developer/article/1456966

数值法，了解了：L1正则化，对于某些i，如果 $|L'(w_i)|<c$ ，那这个 $w_i$ 一定会取到0，求导就能发现，

$$
L(w_i)+C*|w_i|
$$

求导，得到

$$
\begin{cases}
L'(w_i)-c & w_i<0 \\
L'(w_i)+c & w_i>0 
\end{cases}
$$

导数在 $w_i >0$ 也大于0，而导数在 $w_i <0$ 也小于0。$w_i =0$ 的时候能取到最小。
L2 正则化没有这种规律。


解空间的形状，了解了：
https://www.zhihu.com/question/35508851
腾讯的人的回答。
损失函数是凸函数，在2个参数的情况下，
损失值的等高线容易和 L1正则化函数的菱形等高线的脚点相交，因为脚点是凸起来的点（于是会有一个参数是0），但是和L2正则化函数的圆形等高线相交，每个点都有可能。



### 从数学角度解释L2为什么能提升模型的泛化能力
https://www.zhihu.com/question/35508851




### 为什么L1正则化相当于对模型参数w引入了拉普拉斯先验，L2正则化相当于引入了高斯先验？

拉普拉斯先验的极值是平的，有很大概率取到0，而高斯分布的极值是尖的，（不知道为什么就容易取到0了，极值就一定是0 吗？）

