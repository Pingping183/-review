# cost function
loss function 是cost function在单样本上的特例
cost function 是求和

## mean square loss
Mean Square Loss，即均方误差（Mean Squared Error，MSE），是一种常用的损失函数，特别是在回归问题中。它用于衡量预测值与真实值之间的差异程度。MSE的计算公式为预测值与真实值之差的平方的平均值，即：

$$MSE=\frac{1}{n}\sum_{i=1}^n(y_i - \hat{y_i})^2$$

其中，n是样本数量，yi是真实值，yi^是预测值。

MSE的值越小，说明预测模型描述实验数据具有更好的精确度。这是因为MSE通过平方的方式放大了预测值与真实值之间的差异，使得较大的误差对MSE的影响更大，从而促使模型在训练过程中更加关注这些较大的误差，以提高整体的预测精度。

然而，MSE也存在一些缺点。例如，当预测值接近0或1时，MSE的偏导值可能会变得非常小，这可能导致模型在训练初期的学习速率非常慢。此外，MSE对异常值（即远离大多数数据点的值）比较敏感，因为异常值会导致MSE的值显著增加。

尽管存在这些缺点，但MSE仍然是一种广泛使用的损失函数，特别是在回归问题中。在实际应用中，可以根据具体问题的特点和需求选择合适的损失函数来优化模型。

# 向量间计算关系
## 向量之间的距离
https://blog.csdn.net/ljyljyok/article/details/81509027
### 欧式距离
### 曼哈顿距离
|x1-x2|+|y1-y2|
### 切比雪夫距离
max(|x1-x2|, |y1-y2|)


### 闵可夫斯基距离（闵氏距离）
$$d(R_i,R_j) =\sqrt[r]{\sum^n_{k=1}(|R_{ik}-R_{jk}|)^p}$$
特殊：
r=p=1,曼哈顿距离
r=p=2,欧式距离
从横向(各维)看，它等同的看待了不同不同的分量，这种缺陷从切比雪夫距离中可以明显看出，忽略了不同维的差异。从纵向(单维)看，它忽略了不同维的各对象的分布差异

### 加权欧式距离
含义：按照均值和方差标准化之后的欧式距离。
$$d(U_i,U_j) = \sqrt{\sum_{k=1}^n(\frac{U_{ik}-U_{jk}}{s_k})^2}$$
因为欧式距离需要分量相减，所以相当于没有减均值，只是除以了标准差

### 马氏距离
即数据的协方差距离，与欧式距离不同的是1）它考虑到不同属性的方差大小有差异，这个加权欧式距离已经考虑到了 2）还有各属性之间的联系，如考虑性别信息时会带来一条关于身高的信息，因为二者有一定的关联度，而且独立于测量尺度。
$$d(x,y) = \sqrt{(x-y)'\sum^{-1}(x-y)}$$
$\sum$ 是向量==分量==之间的协方差，比如两个向量x=(a1,b1,c1),y=(a2,b2,c2),那么协方差矩阵的非对角线就是(a1,a2,a3,...)和(b1,b2,b3),(c1,c2,c3,...)两两的协方差，对角线就是(a1,a2,a3,...)等等的方差。
理解一下：
a. 分量的方差越大的，计算马氏距离时占比会变小,也就是方差归一化，举个例子，虽然身高cm比性别0/1的方差大，但是两者同等重要，如果用欧式距离，身高会在距离计算中占主导。但用马氏距离，假设两者之间协方差是0，那么马氏距离就等于
$$d(x,y) = \sqrt{\frac{（a_1-a_2)^2}{var(a)} + \frac{(b_1-b_2)^2}{var(b)} +...}$$
方差越大的占比越小，相当于大家都除以自己的标准差，再计算距离：
$$d(x,y) = \sqrt{(\frac{a_1}{std(a)}-\frac{a_2}{std(a)})^2+...}$$
b. 分量之间协方差越大的，相关性高，信息重复了，在距离计算时占比会小
协方差的矩阵的逆不直观，就理解一下吧。不举例了。

>向量之间的协方差： $cov(a,b) = \frac{1}{m-1}\sum^{m}_{i=1}(a_i-\mu_a)(b_i-\mu_b)$ 

### 余弦距离
含义：两个空间向量的夹角余弦值。也叫`余弦相似度`。当两个向量方向完全相同的时候，夹角为0，余弦值为1， 相反的时候，余弦值是-1.计算公式： 
$$d(s_i,s_j) = cos(\vec{s_i},\vec{s_j}) = \frac{\sum_{k=1}^ns_{ik}s_{jk}}{\sqrt{\sum^n_{k=1}s_{ik}^2\sum^n_{k=1}s_{jk}^2}}=\frac{\vec{s_i}\vec{s_j}}{||s_i||\*||s_j||}$$  
remark 向量点积计算公式：  
$$\vec{s_i}\vec{s_j} = ||s_i||*||s_j||*cos<\vec{s_i},\vec{s_j}> = \sum_{k=1}^ns_{ik}s_{jk}$$  
 
应用：判断评分的相似度（类似于归一化比较，偏好相似度），比如一个人对两道菜的评分是2,3，另一个人对两道菜的评分是4,6，他们的余弦距离(2,3),(4,6)就是1，那么他们对这两道菜的偏好是一样的。
修正的余弦相似度：
按均值和标准差标准化之后，得到：
$$r_{i,j}$$

### 皮尔森相关系数
可以描述为不同对象偏离拟合的中心线程度，可以进一步的理解为，许多对象的属性拟合成一条直线或者曲线，计算每个对象相对于这条线的各属性偏离程度.缺失的属性就不计入计算了。
$$\rho_{X,Y} = \frac{cov(X,Y)}{\sqrt{\sigma(X)\sigma(Y)}}=\frac{E(XY)-E(X)E(Y)}{\sqrt{EX^2-(EX)^2} \sqrt{EY^2-(EY)^2}}$$
再用样本均值代替期望就可以计算样本的相关系数：
$$\rho_{X,Y} = \frac{\sum{XY}-\frac{\sum{X}\sum{Y}}{N}}{\sqrt{(\sum{X^2}-\frac{(\sum{X})^2}{N})(\sum{Y}^2-\frac{(\sum{Y})^2}{N})}}$$
其中，N是，x,y共同属性的数量
实操注意：
a.有缺失值
只取交集，计算时应该去掉有缺失值的属性，只留下公共属性。
https://blog.csdn.net/ifnoelse/article/details/7765984
比如两个用户共同观看了200部电影，虽然不一定给出相同或完全相近的评分，他们之间的相似度也应该比另一位只观看了2部相同电影的相似度高吧！但事实并不如此，如果对这两部电影，两个用户给出的相似度相同或很相近，通过皮尔森相关性计算出的相似度会明显大于观看了相同的200部电影的用户之间的相似度。用python计算person相关系数

    from scipy.stats import pearsonr
    x = (5, 2.5, 2,3,2,3)
    y = (5, 3, 2,3,2,3)
    z = (5, 3, np.nan,3,2,3)#有nan会输出nan，得手动删nan
    # 计算皮尔森相关系数
    correlation, p_value = pearsonr(x, y)
    print("皮尔森相关系数:", correlation)#0.98
    print("p值:", p_value)
### mahout-pearson
mahout对皮尔森相关系数的修正，具体怎么修正的还不知道。
应用：在Mahout中，皮尔森相关系数被用于计算用户之间的相似度。然而，它有两个主要的缺点：一是没有考虑用户间重叠的评分项数量对相似度的影响；二是如果两个用户之间只有一个共同的评分项，相似度也不能被计算。为了解决这些问题，Mahout在构造PearsonCorrelationSimilarity时提供了Weighting.WEIGHTED参数，使得有更多相同评分项目的用户之间的相似度更趋近于1或-1。
怎么用的也看得不太明白。先放在这：

    from gensim.models import Word2Vec
    from gensim.similarities.index import AnnoyIndexer

    # 假设你有一个训练好的Word2Vec模型
    model = Word2Vec.load('path_to_your_model')

    # 创建一个Annoy索引器
    indexer = AnnoyIndexer(model, 10)

    # 计算两个向量之间的相似度
    similarity = indexer.similarity(vector1, vector2)

    print("相似度:", similarity)
    在这个示例中，PearsonCorrelationSimilarity
    函数通过AnnoyIndexer类来计算两个向量之间的相似度。
    你可以根据需要调整AnnoyIndexer的参数，例如num_trees（树的数量）。

## 字符之间的距离
### 汉明距离
在信息论中，表示为两个「等长」字符串之间对应位置的不同字符的个数。换句话说，汉明距离就是将一个字符串变换成另外一个字符串所需要「替换」的字符个数。如下：
0110与1110之间的汉明距离是1；
0100与1001之间的汉明距离是3；

### Jaccard相似度
Jaccard相似度常用于二值型数据的相似度计算，是汉明距离的进一步应用。
$$J(A,B) = \frac{|A\bigcap{B}|}{|A\bigcup{B}|}$$
其中，A,B 是集合
在数据挖掘中，经常将属性值二值化，通过计算Jaccard相似度，可以简单快速的得到两个对象的相似程度。
当然，对于不同的属性，这种二值型程度是不一样的，比如，在中国，熟悉汉语的两个人相似度与熟悉英语的两个人相似度是不同的，因为发生的概率不同。所以通常会对Jaccard计算变换，变换的目的主要是为了拉开或者聚合某些属性的距离

    def jaccard_sim(a, b):
        unions = len(set(a).union(set(b)))#求并集
        intersections = len(set(a).intersection(set(b)))#求交集
        return intersections / unions
    
    a = ['x', 'y']
    b = ['x', 'z', 'v']
    print(jaccard_sim(a, b))


# 欧几里得数据类型
欧几里得数据类型是在欧式空间中表示的数据，具有 $\textbf{平移不变性和排列整齐}$ 的特点，常见于图像、文本等规则结构的数据中。
图是一种非欧几里得数据类型，它存在于三维空间，不像其他数据类型如图像、文本和音频。由于图不具备平移不变性，卷积神经网络等传统方法无法有效提取其结构信息，因此衍生出了处理这类数据的网络，即图神经网络。（图论中的图）

# 详解最大似然估计（MLE）、最大后验概率估计（MAP），以及贝叶斯公式的理解

这两种是常见的参数估计的方法
MAP和MLE是统计的内容（是知道数据去推模型，参数）

 $P(X|\theta)$ 
1. 若X已知， $\theta$ 未知，则是似然函数（根据现有的X这个样本，选一个 $\theta$ ，让这样的X出现的概率最大）--不足是这样的X不一定反映了真实的 $\theta$ ，因此需要知道 $P(\theta) \theta$ 的先验概率，乘以 $P(\theta)$ ，使这个值最大，才是真实的 $\theta$ 的值。也就是最大化 $P(X|\theta)P(\theta)$ .这个就是MAP。（需要预先知道 $P(\theta)$ 的信息)

2. 若X未知，$\theta$ 已知，则是概率函数。

参考文件

https://blog.csdn.net/u011508640/article/details/72815981

# pc算法因果网络
只是构建因果图，有图有箭头，没有数值。
通过相关性可以弄出图，但是没有箭头，通过这个算法可以弄出箭头。

## PC算法在因果网络中的应用
PC算法是因果发现中常用的一种方法，基于概率图模型的理论，通过分析变量之间的条件独立性来推断因果关系。
PC方法包括两个基本步骤：图的构建和图的修剪。
图的构建阶段：基于数据中的变量之间的相关性构建一个初始的无向图。然后使用条件独立性测试来检验变量之间的独立性关系，从而将图中的边进行有向化。
图的修剪阶段：从初始图中去除不可能的因果关系，以得到一个更精确的因果图。这个阶段使用启发式算法，如PC算法、FSPC算法等，来根据变量之间的条件独立性关系判断是否可以去除某些边。

## PC算法的优点与局限性
优点：PC算法可以在实际应用中进行因果推断，而不需要进行实验，这使得它在许多领域具有优势。适用于高维稀疏因果图。
局限性：PC算法对数据的要求比较高，需要大量的样本数据来进行准确的推断。此外，面对复杂的因果关系时，PC算法可能会产生错误的推断结果。

## 寻找DAG (direct acyclic graph)的等价类
==概念==：
如果能从G图中的d-seperation推断出分布是条件独立的，那么就说这个分布对G是faithful的。反之亦然（如果分布对G是faithful的，那么就能从G图中的d-seperation推断出分布是条件独立的）
==在给定V中其他某些变量向量的情况下,i,j结点的向量是独立的==P相对于G是忠诚的（P 是所有结点的分布）
结点是随机向量x的分量，X是一个p维的向量，X的每个分量取值都是实数。所以每个结点的取值都是实数。



## 辛普森悖论
如果忽略了第三个变量（如性别）=，那结论会完全相反---不能忽略性别的因



# 降维

## 主成分分析（PCA）

## 多维缩放(MDS)

## 线性判别分析(LDA)

## 等度量映射(Isomap)

## t-sne
t-distributed Stochastic Neighbor Embedding，翻译为 t分布-随机邻近嵌入
详见：C:\Users\m00842394\Desktop\vscodelearning\t-sne原理.md
这里有代码：https://www.datacamp.com/tutorial/introduction-t-sne
参考材料：Visualizing Data using t-SNE，2008年发表在Journal of Machine Learning Research，大神Hinton的文章：http://www.jmlr.org/papers/v9/vandermaaten08a.html
参考资料：https://cloud.tencent.com/developer/article/1549992

t-SNE 让您直观地了解数据在高维空间中的排列方式。它通常用于将复杂数据集可视化为二维和三维空间，让我们能够更好地了解数据中潜在的模式和关系。

### 步骤
t-SNE 模拟一个点在高维和低维中被选为另一个点的邻居。它首先使用`高斯核`计算高维空间中所有数据点之间的成对相似性。相距较远的点被选中的概率低于相距较近的点。   
然后，该算法尝试将高维数据点 映射 到低维空间，同时保留成对的相似性。   
它是通过最小化原始高维和低维概率分布之间的`Kullback-Leibler (KL) 散度`来实现的。该算法使用`梯度下降`来最小化散度。低维嵌入被优化到稳定状态。  

对于 t-SNE 算法来说，`困惑度`是一个非常重要的超参数，它控制着每个点在降维过程中考虑的有效邻居数量。 


SNE是通过`仿射`(affinitie)变换将数据点映射到概率分布上，主要包括两个步骤：



>t-SNE将数据点之间的相似度转化为条件概率，原始空间中数据点的相似度由高斯联合分布表示，嵌入空间中数据点的相似度由学生t分布 表示。

>在文中提到的论文中，主要讨论降维出现的拥挤问题，解决的方法也很巧妙，一旦理解它后就明白为什么叫t-分布随机近邻嵌入。

如果想象在一个三维的球里面有均匀分布的点，不难想象，如果把这些点投影到一个二维的圆上一定会有很多点是重合的。

所以，为了在二维的圆上想尽可能表达出三维里的点的信息，大神Hinton采取的方法：

把由于投影所重合的点用不同的距离（差别很小）表示。

这样就会占用原来在那些距离上的点，原来那些点会被赶到更远一点的地方。

t分布是长尾的，意味着距离更远的点依然能给出和高斯分布下距离小的点相同的概率值。

从而达到高维空间和低维空间对应的点概率相同的目的。

通过原始空间和嵌入空间的联合概率分布的KL散度（用于评估两个分布的相似度的指标，经常用于评估机器学习模型的好坏）来评估嵌入效果的好坏。

也就是，将有关KL散度的函数作为损失函数（loss function），通过梯度下降算法最小化损失函数，最终获得收敛结果。

### 对比pca
t-SNE 与 PCA
t-SNE 和 PCA 都是降维技术，它们具有不同的机制，并且适用于不同类型的数据。

PCA（主成分分析）是一种线性技术，最适合用于具有线性结构的数据。它试图通过投影到较低维度、最小化方差和保留较大的成对距离来识别数据中的潜在主成分。阅读我们的主成分分析 (PCA)教程，通过 R 示例了解算法的内部工作原理。 

但是，t-SNE 是一种非线性技术，专注于保持低维空间中数据点之间的成对相似性。t-SNE 关注的是保持较小的成对距离，而 PCA 则专注于维持较大的成对距离以最大化方差。

总结一下，PCA保留了数据中的方差，而t-SNE保留了低维空间中数据点之间的关系，因此它是一种很好的复杂高维数据可视化算法。 

### 应用
该算法在论文中非常常见，主要用于高维数据的降维和可视化。

除了可视化复杂的多维数据之外，t-SNE 还有其他用途，主要用于医学领域。

t-SNE变换后，如果在低维空间中具有可分性，则数据是可分的；如果在低维空间中不可分，则可能是因为数据集本身不可分，或者数据集中的数据不适合投影到低维空间。



聚类和分类：在低维空间中将相似的数据点聚类在一起。它也可以用于分类和查找数据中的模式。 
异常检测：识别数据中的异常值和异常。 


# 梯度下降算法
参考资料：https://www.cnblogs.com/pinard/p/5970503.html

无约束优化算法的一种，其他还有最小二乘法，此外还有牛顿法和拟牛顿法。
梯度下降法就是对参数求导，参数(t) = 参数（t-1）-step * d参数
d参数 = 函数对参数求微分
找到最优的参数，判断方法：

因为不是单纯的递增或者递减，某参数的变动和过去的该参数，过去的其他参数有关，每次你这个方向动一点（可能别的参数不变的话，单看你的变化对整体是增加），其他参数方向动一点，才能让整体下降最快。

## 和最小二乘、牛顿法的区别：
梯度下降法和最小二乘法相比，梯度下降法需要选择步长，而最小二乘法不需要。梯度下降法是迭代求解，最小二乘法是计算解析解。如果样本量不算很大，且存在解析解，最小二乘法比起梯度下降法要有优势，计算速度很快。但是如果样本量很大，用最小二乘法由于需要求一个超级大的逆矩阵，这时就很难或者很慢才能求解解析解了，使用迭代的梯度下降法比较有优势。

梯度下降法和牛顿法/拟牛顿法相比，两者都是迭代求解，不过梯度下降法是梯度求解，而牛顿法/拟牛顿法是用二阶的海森矩阵的逆矩阵或伪逆矩阵求解。相对而言，使用牛顿法/拟牛顿法收敛更快。但是每次迭代的时间比梯度下降法长。

## 梯度下降的调优
如果损失函数不是凸函数，会受到初始值、步长的影响，只能得到局部最优解，所以初始值、步长可以调整
迭代速度可以调优

### AdaGrad的梯度下降算法
学习曲率

### 随机梯度下降
stochastic gradient descent SGD
每次优化的样本随机取几个，防止样本量太大导致计算太慢。

# 牛顿法
优化算法之一。牛顿法其实很简单，就是通过切线与x轴的交点不断更新切线的位置，直到达到曲线与x轴的交点得到方程解。

在实际应用中我们因为常常要求解凸优化问题，也就是要求解函数一阶导数为0的位置，而牛顿法恰好可以给这种问题提供解决方法。

实际应用中牛顿法首先选择一个点作为起始点，并进行一次二阶泰勒展开得到导数为0的点进行一个更新，直到达到要求，这时牛顿法也就成了二阶求解问题，比一阶方法更快。我们常常看到的x通常为一个多维向量，这也就引出了Hessian矩阵的概念（就是x的二阶导数矩阵）。

缺点：牛顿法是定长迭代，没有步长因子，所以不能保证函数值稳定的下降，严重时甚至会失败。还有就是牛顿法要求函数一定是二阶可导的。而且计算Hessian矩阵的逆复杂度很大。
为了求损失函数的最小值，把损失函数用泰勒展开到2阶，然后求导，令=0，求出x的解，带入。此处认为x是k+1时刻的x, $x_0$ 是k时刻的x.从 $x_0$ 挪一步变成 $x$ ，继续迭代，直到f(x)的变化小于规定值。
把f(x)用泰勒展开到二阶：

$$
f(x) = f(x_0) + f'(x_0)(x-x_0)+\frac{1}{2}f''(x_0)(x-x_0)^2
$$

f(x)对x求导，令其为0，得到x的解。

$$
f'(x) = f'(x_0)+f''(x_0)(x-x_0) = 0 \\
x = x_0-\frac{f'(x_0)}{f''(x_0)}
$$

也即，

$$
x_{k+1} = x_k - \frac{f'(x_0)}{f''(x_0)}
$$

直到 $f(x_{k+1}-f(x_k))$ 小于 $\epsilon$ 


[![牛顿法](image-46.png)](https://blog.csdn.net/google19890102/article/details/41087931)
## 拟牛顿法
不用二阶偏导而是构造出Hessian矩阵的近似正定对称矩阵的方法称为拟牛顿法。拟牛顿法的思路就是用一个特别的表达形式来模拟Hessian矩阵或者是他的逆使得表达式满足拟牛顿条件。主要有DFP法（逼近Hession的逆）、BFGS（直接逼近Hession矩阵）、 L-BFGS（可以减少BFGS所需的存储空间）。

# 交叉验证
训练集和测试集的划分方法会影响模型以及模型的结果  
而且不能充分利用数据，（测试集的数据没法用来训练）

## 1.模型的评价标准
如果是回归问题是对每次得到的MSE取平均。（如果是分类问题，就是错误分类个数的平均）

## 2.用途
交叉验证的这个手段一般可用于模型的“选择”，通过交叉验证后学习器的综合表现来选择不同的模型和超参数。
选了模型之后调参，使得MSE最小。
>特，可以比较两个模型的好坏
>数值检验1.对每个折训练两个模型，得到误差，做成对t检验，检验误差均值是不是显著不同。成对t检验要求数据满足正态分布的假设（或至少差异 $d_i$ 接近正态分布）。如果数据不满足这一假设，可能需要考虑使用非参数检验（如Wilcoxon符号秩检验）
>分类检验2.
>2.或者用McNemar 检验， 是 2 * 2 表的配对检验.

## 3.LOOCV
每次只取一个数据作为测试数据，剩下的作为训练集，对每次得到的MSE取平均。（或者错误分类个数的平均如果是分类问题）
不受划分的影响，模型的Bias是最小的，但是耗时大。----》
为了解决计算成本太大的弊端，又有人提供了下面的式子，是的LOOCV计算成本和只训练一个模型一样快。

$$
CV(n) = \frac{1}{n}\sum_{i=1}^n(\frac{y_i-\hat(y_i)}{1-h_i})^2
$$

其中 $\hat{y_i}$ 表示拟合值，而 $h_i$ 表示leverage, 关于 $h_i$ 的计算方法详见线性回归的部分。
这个没懂

## 4.K折

折K次进行交叉验证，对得到的MSE 取平均
K的取值：
K越大，每次投入的训练集的数据越多，模型的Bias越小。但是K越大，又意味着每一次选取的训练集之前的相关性越大（考虑最极端的例子，当k=N，也就是在LOOCV里，每次都训练数据几乎是一样的）。而这种大相关性会导致最终的test error具有更大的Variance。（没懂偏倚和test error)

一般来说，根据经验我们一般选择k=5或10。

# 逻辑回归
逻辑回归的cost function 不用平方误差，因为平方误差是非凸的，一般用

$$
J(w,b) = \frac{1}{m}\sum^m_{i=1}L(\hat{y}^{(i)},y^{(i)}) =
-\frac{1}{m} \sum^m_{i=1}[ -(y^{(i)}log\hat{y}^{(i)} + (1-y^{(i)}) log(1-\hat{y}^{(i)}))]
$$

也是：条件对数似然，如果这样理解拟合函数 ： $\hat{y} = P(y=1|x)$
## 逻辑回归调优
梯度下降、随机梯度下降、mini 随机梯度下降降法。随机梯度下降不但速度上比原始梯度下降要快，局部最优化问题时可以一定程度上抑制局部最优解的发生。

## 逻辑斯特回归为什么要对特征进行离散化。
非线性！非线性！非线性！逻辑回归属于`广义线性模型`，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合； 离散特征的增加和减少都很容易，易于模型的快速迭代；
>'广义线性模型`


速度快！速度快！速度快！稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；
鲁棒性！鲁棒性！鲁棒性！离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；

方便交叉与特征组合：离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力；

稳定性：特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问；


简化模型：特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。

##  逻辑回归的目标函数中增大L1正则化会是什么结果。
所有的参数w都会变成0。



# 正则化
参考：https://cloud.tencent.com/developer/article/1456966

正则化是一种用来改善模型泛化能力，减少过拟合的技术。
>过拟合：在训练集上表现好，但是测试集上表现不好。原因：数据选择：训练集不能代表总体，有很强的误导性，或者模型太复杂，像是为训练集量身定制一般，所以在测试集上就不适用了。



目标函数（通常是最小化损失函数）不仅考虑了模型对训练数据的拟合程度，还可能会包含正则化项以防止模型过拟合。

常见的惩罚项有L0、L1和L2惩罚项，
其中L0惩罚项为权值向量W中不为0的分量个数，
L1惩罚项为权值向量W各分量的绝对值之和，（L1正则化也是绝对值之和）
这两个惩罚项皆可以很好地维持权值W的稀疏性。
单目标值时，L2惩罚项为权值向量W的模，（平方和开根号），叫做L2惩罚项（L2正则化是不开根号的，但是都是和平方和有关的）
多目标值时，L2惩罚项为权值矩阵W的奇异值的最大值，L2惩罚项可以很好地防止模型过拟合。

## L0正则化

既然L0可以实现稀疏，为什么不用L0，而要用L1呢？个人理解一是因为L0范数很难优化求解（NP难问题），二是L1范数是L0范数的最优凸近似，而且它比L0范数要容易优化求解。所以大家才把目光和万千宠爱转于L1范数。


## L1正则化（也称为Lasso正则化）

是在目标函数中添加一个关于模型参数绝对值的和（即参数向量的L1范数）的惩罚项。增大L1正则化的强度意味着在目标函数中增加这个惩罚项的权重，这会导致模型在训练过程中更倾向于选择参数绝对值较小的解，甚至使某些参数变为0（因此具有特征选择的效果），从而使模型变得更加简单和稀疏。

具体来说，在逻辑回归中，假设我们的目标是最小化交叉熵损失函数 L(w,b)（其中 w 是权重，b 是偏置项），那么加上L1正则化后的目标函数可以表示为：

$$
J(w,b) = L(w,b) + \lambda \sum^n_{i-1}|w_i|
$$

其中，λ 是正则化强度系数，它控制了正则化项在目标函数中的影响程度。n 是权重的数量（不包括偏置项 b，因为通常不对偏置项进行正则化）。

增大 λ 的值意味着增大L1正则化的强度，这会导致模型在训练过程中更加注重减小参数的绝对值之和，进而可能使得某些权重 $w_i$ 

​变为0，从而达到稀疏化的效果。稀疏化有助于减少模型的复杂性，避免过拟合，同时也可能使模型更加容易解释（因为很多特征对应的权重为0，意味着这些特征在预测时不起作用）。

然而，需要注意的是，正则化强度的选择是一个重要的超参数调优过程，过大的 λ 值可能导致模型欠拟合，即模型无法充分学习到数据的特征，导致在训练集和测试集上的表现都很差。因此，在实际应用中，需要通过交叉验证等方法来选择合适的 λ 值。

## L2正则化
（也称为权重衰减、岭回归（Ridge Regression）或Tikhonov正则化）是一种在机器学习和深度学习中广泛使用的正则化技术，旨在减少模型的复杂度，防止模型过拟合，并提高模型的泛化能力。L2正则化通过在损失函数中添加一个对模型权重平方（L2惩罚项一般是权重平方和开根号，有时候也不用开根号，反正跟权重平方和有关就行了）的惩罚项来实现这一目标。

### 基本原理
在没有正则化的情况下，模型的损失函数（如均方误差、交叉熵等）通常只包含与数据拟合度相关的项。加入L2正则化后，损失函数会变为：


$$
L(w)=L _0(w) + \frac{\lambda}{2n}\sum_jw_j^2
$$

其中：
 $L_0(w)$ 是原始损失函数，w 是模型权重。

λ 是正则化强度或正则化系数，用于控制正则化项对总损失的影响程度。较大的 λ 值会导致更强的正则化效果，即更小的权重值。

n 是训练样本的数量，这个因子通常用于归一化，但在实践中，有时为了简化计算会省略。

$\sum_jw_j^2$ 是权重向量的L2范数（即权重的平方和），它惩罚了较大的权重值，倾向于使权重更加分散，避免模型过于依赖某些特征。



>岭回归和l2范数，ridge
回顾一下最小二乘法：https://www.cnblogs.com/wellp/p/9286240.html

$$
\hat{y} = X\beta \\
w^* = argmin_w \sum_{i = 1}^n(\hat{y}-y)^2
$$

假设样本格式是n，特征个数是p，那么 $y_{(n*1)}$ , $x_{n*2}$ , $\beta_{n*2}$ 
ps:X第二列是1， 有一个常数项
损失函数C对 $\beta$ 求导，用矩阵表示

$$
C(\beta) = ( x \beta-y)'(x \beta -y) = \beta' x' x \beta -y'x\beta- \beta'x'y +y'y \\
$$

令：

$$
\frac{\partial C}{\partial \beta} = ((x'x)'+x'x)\beta -2x'y = 0 \\
 2x'x \beta -2x'y = 0 \\
解得：\beta^* = (x'x)^{-1}x'y  \\
$$

$$
remark: \frac{\partial x'Ax}{\partial x} = (A'+A)x ,   
\frac{\partial x'A}{\partial x} = \frac{\partial A'x}{\partial x} =  A
$$

岭回归：
目的：在求解参数的时候，为了解的稳定性，因为不稳定的解（矩阵），会导致，虽然输入的x差不多，但是输出的y差很多
而矩阵的这种稳定性用条件数来表示，条件数太大会导致这个矩阵不稳定
于是在损失函数里面加了一个L2正则项，让这个x'x 变成 $(x'x+\lambda I)$ 矩阵一定满秩，==一定可逆。如果不可逆的话，通过这个解，样本x对y的预测会很不稳定，而这个规则项的引入则可以改善条件数。
<img width="559" alt="image-47" src="https://github.com/user-attachments/assets/734e3cef-5b74-46c6-8807-38863a9916ba">
于是损失函数变成 $C(\beta) = ( x \beta-y)'(x \beta -y) + \lambda \beta'\beta$ 
求导得到解：$\beta^* = (x'x+\lambda I)^{-1} x'y$ 

### 作用
防止过拟合：通过限制权重的大小，L2正则化可以减少模型的复杂度，防止模型在训练数据上学习得过于精细，从而避免在未见过的数据上表现不佳（即过拟合）。
提高泛化能力：较小的权重值意味着模型在输入数据上的变化不会过于敏感，从而提高了模型在不同数据上的表现能力，即泛化能力。（为什么）
数学上的稳定性：L2正则化使得损失函数成为强凸函数，这有助于在优化过程中找到全局最优解，并减少优化算法的波动。
单目标的L2惩罚项限制了参数向量的模，使每个分量都小于一定范围，分量之间的差异性会减弱。而训练的过程会使分量之间的差异变大（为了找到有用的特征），参数向量的分量差异会增大，这个回归考虑到了两者之间的平衡。
>关于多目标l2惩罚项的意义：参见https://www.zhihu.com/question/35508851

## L1和L2的区别
L1的功能是使权重稀疏，而L2的功能是使权重平滑。

### L1正则为什么可以得到稀疏解？L2得到权重平滑的解？
参考材料：
https://cloud.tencent.com/developer/article/1456966

数值法，了解了：L1正则化，对于某些i，如果 $|L'(w_i)|<c$ ，那这个 $w_i$ 一定会取到0，求导就能发现，

$$
L(w_i)+C*|w_i|
$$

求导，得到

$$
\begin{cases}
L'(w_i)-c & w_i<0 \\
L'(w_i)+c & w_i>0 
\end{cases}
$$

导数在 $w_i >0$ 也大于0，而导数在 $w_i <0$ 也小于0。$w_i =0$ 的时候能取到最小。
L2 正则化没有这种规律。


解空间的形状，了解了：
https://www.zhihu.com/question/35508851
腾讯的人的回答。
损失函数是凸函数，在2个参数的情况下，
损失值的等高线容易和 L1正则化函数的菱形等高线的脚点相交，因为脚点是凸起来的点（于是会有一个参数是0），但是和L2正则化函数的圆形等高线相交，每个点都有可能。



### 从数学角度解释L2为什么能提升模型的泛化能力
不容易过拟合。
如果发生过拟合， 参数θ一般是比较大的值，（不知道为什么） 加入惩罚项后， 只要控制λ的大小，当λ很大时，θ1到θn就会很小，即达到了约束数量庞大的特征的目的。
模型过于复杂是因为模型尝试去兼顾各个测试数据点， 导致模型函数如下图，
![image](https://github.com/user-attachments/assets/8a5d6767-a3d4-47d7-9503-39e1fd7e0a5f)
处于一种动荡的状态， 每个点的到时在某些很小的区间里，函数值的变化很剧烈。这就意味着函数在某些小区间里的导数值（绝对值）非常大，由于自变量值可大可小，所以只有系数足够大，才能保证导数值很大。
链接： https://blog.csdn.net/NXHYD/article/details/104650246
单目标值L2惩罚项表示为权值向量W的模的大小，当线性模型的代价函数中加入单目标值L2惩罚项后，一方面，为了更好地符合训练数据，学习的本质促使各特征之间的差异性增大，即权值向量W的各分量之间的差异增大；另一方面，为了满足惩罚项，权值向量W的模必须受限小于一定范围，也就意味着权值向量W的每个分量都受限小于一定范围，分量之间的差异性就不会过于明显。如此以来，我们可以用“瞻前顾后”来形容带惩罚项的线性模型的训练过程。
链接：https://www.zhihu.com/question/35508851/answer/110308255



### 为什么L1正则化相当于对模型参数w引入了拉普拉斯先验，L2正则化相当于引入了高斯先验？

拉普拉斯先验的极值是平的，有很大概率取到0，而高斯分布的极值是尖的，（不知道为什么就容易取到0了，极值就一定是0 吗？）
最小化损失函数的形式（L2/L1正则化）==最大化后验概率的表达式（w服从拉普拉斯分布/高斯分布）
参考材料：https://cloud.tencent.com/developer/article/1453447
