transformer部分的截图来源于大模型的课程笔记：
https://3ms.huawei.com/km/groups/3950972/blogs/details/15466149?moduleId=866852560244334592&ctype=-1


# 神经网络
一手资料：soursera上吴恩达的deeplearning.ai
二手资料：学习笔记 https://baozoulin.gitbook.io/neural-networks-and-deep-learning
git上的资料，有代码：
https://github.com/knazeri/coursera/blob/master/deep-learning/1-neural-networks-and-deep-learning/2-logistic-regression-as-a-neural-network/Logistic%20Regression%20with%20a%20Neural%20Network%20mindset.ipynb


针对不同类型的数据有专门优化过的的神经网络：
* 一般的监督式学习（房价预测和线上广告问题），只要使用标准的`神经网络模型`就可以
* 图像识别处理问题，则要使用`卷积神经网络`（Convolution Neural Network），即CNN
* 处理类似语音这样的序列信号时，则要使用`循环神经网络`（Recurrent Neural Network），即RNN
* 自动驾驶这样的复杂问题则需要更加复杂的`混合神经网络模型`

流程：构建一个深度学习的流程是首先产生Idea，然后将Idea转化为Code，最后进行Experiment。接着根据结果修改Idea，继续这种Idea->Code->Experiment的循环，直到最终训练得到表现不错的深度学习网络模型

## 深度学习
深度学习是机器学习的一种
深度学习中的函数是规模更大，复杂度更高的`深层的神经网络`
深度学习训练过程中用例更大规模的训练数据
没有固定的参数大小，按照经验
一般用网络状的一层一层构建关系
x-f1(x)-f2(f1(x))-f3(f2(f1(x)))-...
抗噪声算法，大模型的参数很多，所以能抗噪声。（多个参数一起回答某个问题不容易跑偏）

深度学习和神经网络之间的关系可以理解为：深度学习是神经网络发展到一定阶段的产物，特别是当神经网络具有多层结构、使用非线性激活函数、并能够在大数据集上进行高效训练时，它就成为了深度学习模型。因此，深度学习是神经网络在结构、算法和应用方面的一个重要拓展和深化。


# 生成式人工智能
不再局限于识别猫还是狗，是更复杂的人工智能
注意：是一个字一个字，一个像素一个像素，一个一个采样点生成
结构比较复杂，因为有非常多的组合方式，比如让机器写一个文章
很复杂，所以现在的生成式人工智能是基于深度学习的神经网络去做的（以前是用规则的）

## 检索增强生成（RAG）  retrieval augmented generation
在不更高模型参数上，通过输入相关的资料的手段来提升大模型应用的准确性
RAG和微调的区别：一个是问询，一个是再训练
![微调](image.png)
![微调2](image-1.png)

数据MRC：把多模异构数据转成机器可读文本（文档版面分析，图表信息提取，音视频内容理解）
语义分片chunking 把知识转化成合适长度的文本片段（固定大小分割，简单意图分割，基于元素与结构的分割）
检索retrieval：连接用户复杂问题和领域知识库
向量模型embedding理解用户复杂问题发挥什么作用：区分多义词的不同含义，聚合近似语义，解决关键字收缩的硬匹配缺陷（电脑坏了==便携故障）
![AI场景应用](image-2.png)
![大模型垂域落地方案](image-3.png)

# 大模型
就是具有非常复杂结构，非常多参数的模型
LLM --大语言模型，以文生文的模型

## 大语言模型
语言模型：给定上下文，计算出下一个
语言模型基本都是transformer的架构


# NLP 
natural language processing
Tf-Idf、Word2Vec和BERT三种模型
22年的项目但是23年才建这个功能，优化都是我们做的
识别问题，认为是自然语言处理的文本分类问题
学习算法，应用算法，训练模型，比较结果，输出
https://www.leiphone.com/category/yanxishe/TbZAzc3CJAMs815p.html
我们有多种技术从原始文本数据中提取信息，并用它来训练分类模型。本教程比较了传统的词袋法（与简单的机器学习算法一起使用）、流行的词嵌入模型（与深度学习神经网络一起使用）和最先进的语言模型（和基于attention的transformers模型中的迁移学习一起使用），语言模型彻底改变了NLP的格局。

## NLP典型模型实操

参考文献：
NLP之——Word2Vec详解
https://www.cnblogs.com/guoyaohua/p/9240336.html
实操内容摘取自：
https://www.leiphone.com/category/yanxishe/TbZAzc3CJAMs815p.html
我们的任务是把这些新闻标题正确分类，这是一个多类别分类问题,数据集链接如下
https://www.kaggle.com/datasets/rmisra/news-category-dataset
词嵌入-如何构建词嵌入embedding matrix模型的原文链接：
https://blog.csdn.net/weixin_60737527/article/details/127015770

代码撸了一遍，放在：
https://huggingface.co/miaomiaolbm/NLP/blob/main/NLP%E4%B8%89%E7%A7%8D%E6%A8%A1%E5%9E%8B%E5%AE%9E%E6%93%8D-checkpoint.ipynb


### 数据预处理
预处理示例，包括清理文本、删除`停用词`以及应用词形还原
>停用词（Stop Words）是指在文本处理中，为了降低索引大小和减少存储空间，同时提高搜索效率而过滤掉的一些常用词汇。这些词汇对于文本的主题理解没有太大帮助，比如英文中的“the”、“is”、“at”、“which”、“on”等，中文中的“的”、“了”、“在”、“是”、“和”等。这些词在语言中非常频繁地出现，但对于文本分析（如信息检索、文本分类、情感分析等）来说，它们并不包含太多有用的信息。然而，需要注意的是，对于某些特定的应用场景，停用词可能并不总是需要被移除。例如，在某些情感分析任务中，一些看似普通的停用词（如“不”）可能对于判断情感倾向至关重要。因此，在决定是否移除停用词之前，应该根据具体的应用场景和需求来仔细考虑。



### 特征工程（词嵌入）word embedding
Word embedding最早出现于Bengio在03年发表的开创性文章中[3]。通过嵌入一个线性的投影矩阵（projection matrix），将原始的one-hot向量映射为一个稠密的连续向量，并通过一个语言模型的任务去学习这个向量的权重。这一思想后来被广泛应用于包括word2vec在内的各种NLP模型中。
>Bengio, Y., Ducharme, R., Vincent, P., & Janvin, C. (2003). A neural probabilistic language model. The Journal of Machine Learning Research, 3, 1137–1155.

Word Embedding的训练方法大致可以分为两类：一类是无监督或弱监督的预训练；一类是端对端（end to end）的有监督训练。

无监督或弱监督的预训练以word2vec和auto-encoder为代表。这一类模型的特点是，不需要大量的人工标记样本就可以得到质量还不错的Embedding向量。不过因为缺少了任务导向，可能和我们要解决的问题还有一定的距离。因此，我们往往会在得到预训练的Embedding向量后，用少量人工标注的样本去fine-tune整个模型。

相比之下，端对端的有监督模型在最近几年里越来越受到人们的关注。与无监督模型相比，端对端的模型在结构上往往更加复杂。同时，也因为有着明确的任务导向，端对端模型学习到的Embedding向量也往往更加准确。例如，通过一个Embedding层和若干个卷积层连接而成的深度神经网络以实现对句子的情感分类，可以学习到语义更丰富的词向量表达。

Word Embedding的另一个研究方向是在更高层次上对Sentence的Embedding向量进行建模。

我们知道，word是sentence的基本组成单位。一个最简单也是最直接得到sentence embedding的方法是将组成sentence的所有word的embedding向量全部加起来——类似于CBoW模型。

显然，这种简单粗暴的方法会丢失很多信息。

另一种方法借鉴了word2vec的思想——将sentence或是paragraph视为一个特殊的word，然后用CBoW模型或是Skip-gram进行训练[12]。这种方法的问题在于，对于一篇新文章，总是需要重新训练一个新的sentence2vec。此外，同word2vec一样，这个模型缺少有监督的训练导向。

个人感觉比较靠谱的是第三种方法——基于word embedding的端对端的训练。Sentence本质上是word的序列。因此，在word embedding的基础上，我们可以连接多个RNN模型或是卷积神经网络，对word embedding序列进行编码，从而得到sentence embedding。

这方面的工作已有很多。有机会，我会再写一篇关于sentence embedding的综述。

#### 离散表示
* One-hot
简称读热向量编码，也是特征工程中最常用的方法。其步骤如下：

构造文本分词后的字典，每个分词是一个比特值，比特值为0或者1。

每个分词的文本表示为该分词的比特位为1，其余位为0的矩阵表示。
* 如何理解`某个单词的one-hots`:
  >* 定义：One-hot编码是一种将类别变量转换为机器学习算法易于处理的形式的方法。在NLP中，它通常用于将单词映射为向量。
  >* 向量表示：对于词汇表中的每个单词，one-hot编码都会生成一个长度为词汇表大小的向量，其中只有一个位置为1，其余位置均为0。这个1的位置对应于单词在词汇表中的索引。
  >* 假设词汇表包含以下单词：["apple", "banana", "cherry"]，则这些单词的one-hot编码分别为：
  apple: [1, 0, 0]
  banana: [0, 1, 0]
  cherry: [0, 0, 1]
  >* 评价One-hot编码在NLP中主要用于早期阶段，当还没有更好的词嵌入技术（如Word2Vec、GloVe等）可用时。然而，随着词嵌入技术的发展，one-hot编码在NLP中的应用已经大大减少，因为它无法有效地捕捉单词之间的语义关系。


* 词袋法
统计一个文章里每个词的出现频率，作为向量，会得到一个巨大的稀疏矩阵缺点是有些词出现的频率很高但是不太影响预测结果

>需要特征选择
为了降低矩阵的维度所以需要去掉一些列，我们可以进行一些特征选择（Feature Selection），这个流程就是选择相关变量的子集。操作如下:
将每个类别视为一个二进制位（例如，"科技"类别中的科技新闻将分类为1，否则为0）;
进行`卡方检验`，以便确定某个特征和其（二进制）结果是否独立;
只保留卡方检验中有特定p值的特征。这将特征的数量从10000个减少到3152个，保留了最有统计意义的特征.选并集

* 词频-逆向文件频率（Tf-Idf）
可以替代单纯的词频，作为单词重要性的表现

* n-gram模型
为了保持词的顺序，做了一个滑窗的操作，这里的n表示的就是滑窗的大小，例如2-gram模型，也就是把2个词当做一组来处理，然后向后移动一个词的长度，再次组成另一组词，把这些生成一个字典，按照词袋模型的方式进行编码得到结果。改模型考虑了词的顺序。

在训练模型之前，需要将语料转换为n元文法列表。具体来说，就是尝试捕获一元文法（"york"）、二元文法（"new york"）和三元文法（"new york city"）。
甚至可以通过某些维度缩减算法（比如`TSNE`），将一个单词及其上下文可视化到一个更低的维度空间（2D或3D）。
![词嵌入的低维映射](image-22.png)
>t-SNE（t 分布随机邻域嵌入）是一种无监督非线性降维技术，用于数据探索和高维数据可视化。非线性降维意味着该算法允许我们分离无法用直线分离的数据。

#### 分布式表示

##### `共现矩阵`

共现矩阵参考材料：https://blog.csdn.net/weixin_42782150/article/details/120646183

参考文献：Turney, P. D., & Pantel, P. (2010). From frequency to meaning: vector space models of semantics. Journal of Artificial Intelligence Research, 37(1)

`Co-Occurrence Matrix` 是具有固定上下文窗口的共现矩阵
>* 与词向量的关系：它不是通常使用的`词向量`。把他用 PCA、SVD 等技术将其分解为因子后，这些因子的组合可以形成`词向量`。
>共现（Co-occurrence）——对于给定的语料库，一对单词（如w1和w2）的共现是指它们在上、下文窗口中同时出现的次数
>上下文窗口（Context Window）——指的是某个单词w的上下文范围的大小，也就是前后多少个单词以内的才算是上下文？一般，上、下文窗口由数字和方向指定。比如说前后1个。
这个矩阵的行是不重复的单词，列也是同样排序的单词，而元素是两个单词之间共现的次数,对角线元素为0。
>* 共现矩阵的优点 
保留了单词之间的语义关系。即男人和女人往往比男人和苹果更亲近。
以 SVD 为核心，产生比现有方法更准确的词向量表示。
使用因式分解，这是一个定义明确的问题，可以有效地解决。
必须计算一次，并且一旦计算就可以随时使用。从这个意义上说，它比其他人更快。
>* 共现矩阵的缺点
它需要巨大的内存来存储共现矩阵。
向量维数随着词典大小线性增长。
存储整个词典的空间消耗非常大。
一些模型如文本分类模型会面临稀疏性问题。
模型会欠稳定，每新增一份语料进来，稳定性就会变化。

##### 向量空间模型VSM
（Vector Space Model，以下简称VSM）
用一个连续的稠密向量去刻画一个word的特征，这样，我们不仅可以直接刻画词与词之间的相似度，还可以建立一个`从向量到概率的平滑函数模型`，使得相似的词向量可以映射到相近的概率空间上。这个稠密连续向量也被称为word的distributed representation。

* 两个重要假设：
  Bag of Words Hypothesis，一篇文档的词频（而不是词序）代表了文档的主题；
  Distributional Hypothesis，上下文环境相似的两个词有着相近的语义

* 如何将稀疏离散的one-hot词向量映射为稠密连续的Distributional Representation？
  1. 基于Bag of Words Hypothesis，构造一个term-document 矩阵A，
  $\begin{bmatrix} * & * \\ * & * \end{bmatrix}$
  矩阵的每个行对应着词典里的一个单词
  矩阵的每个列对应着训练语料里的一篇文档
  矩阵中的每个元素代表单词i在文档j中出现的次数
  所以，可以用行向量作为单词i的语义向量，列向量作为文档j的主题向量

  2. 类似地，我们可以基于Distributional Hypothesis构造一个word-context的矩阵。此时，矩阵的列变成了context里的word，矩阵的元素也变成了一个context窗口里word的共现次数。

* 评价
这种`co-occurrence共现矩阵`仍然存在着数据稀疏性和维度灾难的问题。为此，人们提出了一系列对矩阵进行降维的方法（如LSI／LSA等）。这些方法大都是基于`SVD奇异值分解`的思想，将原始的稀疏矩阵分解为两个低秩矩阵乘积的形式。

>`SVD奇异值分解`主要用于将高维数据集降维，同时保留重要信息
>`PCA分解`PCA分解的作用就是将一个Co-Occurrence 矩阵K分解为三个矩阵，U，S和 V，其中U和V都是正交矩阵。重要的是，U和S的点积表示单词向量，V表示单词上下文。

##### GloVe
参考资料：https://github.com/NLP-LOVE/ML-NLP/blob/master/NLP/16.3%20GloVe/README.md
有代码的
Global Vectors for Word Representation
==基于全局词频统计（count-based & overall statistics）的词表征（word representation）工具==，它可以把一个单词表达成一个由实数组成的向量，这些向量捕捉到了单词之间一些语义特性，比如相似性（similarity）、类比性（analogy）等。我们通过对向量的运算，比如欧几里得距离或者cosine相似度，可以计算出两个单词之间的语义相似性。

* GloVe的共现矩阵
它根据两个单词在上下文窗口的距离 d，提出了一个衰减函数（decreasing weighting）：decay=1/d 用于计算权重，也就是说距离越远的两个单词所占总计数（total count）的权重越小。

根据现有的预料构建：关于词与词之间的距离和频率信息的矩阵之后，把这个矩阵和词的向量联系在一起，再加个bias b, 假设(发明人就这么假设的)
$$
w'_i\hat{w}_j + b_i + \hat{b}_j = log(X_{ij})
$$


* 构造损失函数
$$
J = \sum^V_{i,j=1}{f(X_{ij})(w'_i\hat{w}_j + b_i + \hat{b}_j - log(X_{ij}))^2}
$$
这个cost function的基本形式就是最简单的mean square loss，只不过在此基础上加了一个权重函数$f(X_{ij})$,这个函数的要求有如下三点：
1. 这些单词的权重要大于那些很少在一起出现的单词（rare co-occurrences），所以这个函数要是非递减函数（non-decreasing）；
2. 但我们也不希望这个权重过大（overweighted），当到达一定程度之后应该不再增加；
3. 如果两个单词没有在一起出现，也就是$X_{ij} = 0$，那么他们应该不参与到 loss function 的计算当中去，也就是f(x) 要满足 f(0)=0。

发明人采用了以下的分段函数：
x是矩阵的元素，x越大说明频率越高或者距离越近

$$
f(x) = \begin{cases}
(x/x_{max})^ \alpha   &     x<x_{max}\\
1  &  otherwise
\end{cases}
$$
![权重函数](image-44.png)



* 训练GloVe模型
  
采用了`AdaGrad的梯度下降算法`，对矩阵 X 中的所有非零元素进行随机采样，`学习曲率`（learning rate）设为0.05，在vector size小于300的情况下迭代了50次，其他大小的vectors上迭代了100次，直至收敛。**最终学习得到的是两个vector是$w$ 和$\hat{w}$
，因为 X 是对称的（symmetric），所以从原理上讲$w$ 和$\hat{w}$
 是也是对称的，他们唯一的区别是初始化的值不一样，而导致最终的值不一样。
>为什么一个单词会有两个向量$w$ 和$\hat{w}$，还要让他俩对称，也就是$w'w$是对称矩阵，有要求的。--他就这么设定，然后把两个向量加起来，说是为了提高鲁棒性。

所以这两者其实是等价的，都可以当成最终的结果来使用。但是为了提高鲁棒性，我们最终会选择两者之和 **作为最终的vector（两者的初始化不同相当于加了不同的随机噪声，所以能提高鲁棒性）。**在训练了400亿个token组成的语料后，得到的实验结果如下图所示：
![glove](image-45.png)
这个图一共采用了三个指标：语义准确度，语法准确度以及总体准确度。那么我们不难发现Vector Dimension在300时能达到最佳，而context Windows size大致在6到10之间。


* GloVe与LSA、Word2Vec的比较
LSA（Latent Semantic Analysis）是一种比较早的count-based的词向量表征工具，它也是基于co-occurance matrix的，只不过采用了基于奇异值分解（SVD）的矩阵分解技术对大矩阵进行降维，而我们知道SVD的复杂度是很高的，所以它的计算代价比较大。还有一点是它对所有单词的统计权重都是一致的。而这些缺点在GloVe中被一一克服了。

而word2vec最大的缺点则是没有充分利用所有的语料，所以GloVe其实是把两者的优点结合了起来。从这篇论文给出的实验结果来看，GloVe的性能是远超LSA和word2vec的，但网上也有人说GloVe和word2vec实际表现其实差不多。
>glove有词与词的远近之分，而word2vec目标是为了真实值的预测的条件概率最大。

#### 神经网络表示

##### 神经网络语言模型 NNLM 
(neural network language model)（2003）
实操内容摘取自：
https://www.leiphone.com/category/yanxishe/TbZAzc3CJAMs815p.html

用神经网络建立统计语言模型的框架,首次提出了word embedding的概念
* 基本思想：
1. 假定词表中的每一个word都对应着一个连续的特征向量
2. 假定一个连续平滑的概率模型，输入一段词向量的序列，可以输出这段序列的联合概率
3. 同时学习词向量的权重和概率模型里的参数

* 基本过程
采用一个简单的前向反馈神经网络$$f(w_{t-n+1},...,w_t)$$来拟合一个词序列的条件概率$$p(w_t|w_1,w_2,...,w_{t-1})$$,整个模型的网络结构见下图：
![nnlm](image-33.png)

1. 首先是一个线性的Embedding层。它将输入的N−1个one-hot词向量，通过一个共享的D×V的矩阵C，映射为N−1个分布式的`词向量`（distributed vector）。
 >其中，V是词典的大小(就是一共有多少个词)，D是Embedding向量的维度（一个先验参数）。C矩阵里存储了要学习的word vector。这个矩阵C的列就是词向量（one-hot就是用来取列的，乘了等于没乘）
2. 其次是一个简单的`前向反馈神经网络`g。它由一个`tanh隐层`和一个`softmax输出层`组成。通过将Embedding层输出的N−1个词向量映射为一个长度为V的概率分布向量，从而对词典中的word在输入context下的条件概率做出预估

   $$p(w_t=i|w_1,w_2,...,w_{t-1}) \approx f(w_i,w_{t-1},...,w_{t-n+1}) = g(w_i,C(w_{t-n+1}),...,C(w_{t-1})),i = 1,...,V $$

我们可以通过最小化一个`cross-entropy`的正则化损失函数来调整模型的参数θ：
$$L(\theta) = \frac{1}{T}\sum_tlogf(w_t,w_{t-1},...,w_{t-n+1})+R(\theta)$$

，从t开始往前数一共n个词的函数，取log。再求和。
其中，模型的参数θ包括了Embedding层矩阵C的元素，和前向反馈神经网络模型g里的权重。这是一个巨大的参数空间。不过，在用SGD学习更新模型的参数时，并不是所有的参数都需要调整（例如未在输入的context中出现的词对应的词向量）。计算的瓶颈主要是在softmax层的归一化函数上（需要对词典中所有的word计算一遍条件概率）。

然而，抛却复杂的参数空间，我们不禁要问，为什么这样一个简单的模型会取得巨大的成功呢？

仔细观察这个模型就会发现，它其实在同时解决两个问题：

一个是统计语言模型里关注的条件概率$p(w_t|context)$的计算；

一个是向量空间模型里关注的词向量的表达。

而这两个问题本质上并不独立。通过引入连续的词向量和平滑的概率模型，我们就可以在一个连续空间里对序列概率进行建模，从而从根本上缓解数据稀疏性和维度灾难的问题。

另一方面，以条件概率$p(w_t|context)$为学习目标去更新词向量的权重（就是词向量里面的元素），具有更强的导向性，同时也与VSM里的Distributional Hypothesis不谋而合。

>`cross-entropy`交叉熵：
参考资料https://blog.csdn.net/SongGu1996/article/details/99056721
>`分布式词向量`模型是基于海量语料的监督学习，充分利用语料库中词的上下文相关信息，通过神经网络优化训练语言模型，在此过程中获得`词语的向量化`形式。 这种向量化的分布式表征以“情景语境”为理论基础，通过向量间的夹角余弦相似度来度量词汇的相似度。

3. 评价
NNLM模型只能处理定长的序列



#####  Word2Vec（2013）

参考材料：https://www.cnblogs.com/guoyaohua/p/9240336.html

这些向量是根据每个词出现在另一个词之前或之后的概率分布计算出来的。换一种说法，上下文相同的单词通常会一起出现在语料库中，所以它们在向量空间中也会很接近，例如，考虑"the cat is walking in the bedroom"这句话。如果我们在训练语料中看到了很多类似“the dog is walking in the bedroom”或是“the cat is running in the bedroom”这样的句子，那么，即使我们没有见过这句话，也可以从“cat”和“dog”（“walking”和“running”）之间的相似性，推测出这句话的概率。

* 词嵌入与自编码器:
词嵌入与自编码器有诸多相似之处，它都是将数据维度降低，并且通常以更加合理的方式来表示数据
* 当应用算法来学习词嵌入时，实际上是学习一个嵌入矩阵

>随着深度学习（Deep Learning）在自然语言处理中应用的普及，很多人误以为word2vec是一种深度学习算法。其实word2vec算法的背后是一个浅层神经网络。另外需要强调的一点是，word2vec是一个计算word vector的开源工具。当我们在说word2vec算法或模型的时候，其实指的是其背后用于计算word vector`词向量`的CBoW模型和Skip-gram模型。

Word2Vec有两种不同的方法可以生成`词嵌入向量`：从某一个词来预测其上下文（Skip-gram）`跳词模型`

或根据上下文预测某一个词（Continuous Bag-of-Words）`连续词袋模型`.

其他流行的词嵌入模型还有斯坦福大学的GloVe（2014）和Facebook的FastText（2016）





其他的模型：
###\### Statical language model 统计语言模型
* 解决的问题：
  如何计算一段文本序列在某种语言下出现的概率？比如在机器翻译中，给定输入的原语言句子，从目标语言的集合中选取概率最高的一句话作为翻译的输出。
统计语言模型给出了这一类问题的基本解决框架，对于一段文本序列S：

$$S = w_1,w_2, ..., w_T ，w_i是词$$

这句话出现的概率可以用全概率公式表示为：

$$P(S) = P(w_1, w_2,...,w_T) = \prod_{t=1}^T p(w_T|w_1, w_2,...,w_{t-1})$$

不用1,2,3...t-1那么多的样本去预测t,只用从t开始往前数N个来预测，比如说N=2，I am a student,就用am, a 作为输入，student 作为输出来计算条件概率。可以用词频来估计这个条件概率 (=count(am a student)/count(am a))
>为了避免统计中出现的零概率问题（一段从未在训练集中出现过的Ngram片段会使得整个序列的概率为0），人们基于原始的Ngram模型进一步发展出了
back-off trigram模型（用低阶的bigram和unigram代替零概率的trigram）和
interpolated trigram模型（将条件概率表示为unigram、bigram、trigram三者的线性函数）。
参考资料：Bengio, Y., Ducharme, R., Vincent, P., & Janvin, C. (2003). A neural probabilistic language model. The Journal of Machine Learning Research, 3, 1137–1155.

* 评价
  1. 由于参数空间的爆炸式增长，它无法处理更长程的context（N>3）
  2. 它没有考虑词与词之间内在的联系性。例如，考虑"the cat is walking in the bedroom"这句话。如果我们在训练语料中看到了很多类似“the dog is walking in the bedroom”或是“the cat is running in the bedroom”这样的句子，那么，即使我们没有见过这句话，也可以从“cat”和“dog”（“walking”和“running”）之间的相似性，推测出这句话的概率
  3. Ngram本质上是将词当做一个个孤立的原子单元（atomic unit）去处理的。这种处理方式对应到数学上的形式是一个个离散的one-hot向量



##### 跳词模型skip-gram

它是通过文本中某个单词来推测前后几个单词。例如，根据‘rabbit’来推断前后的单词可能为‘a’,'is','eating','carrot'。
依然是通过一个隐层接一个Softmax激活函数来预测其它词的概率。
![skip-gram](image-31.png)
如果将skip-gram模型的前向计算过程写成数学形式，我们得到：$$p(w_0|W_i) = \frac{e^{U_oV_i}}{\sum_j e^{U_jV_i}}$$
其中，$V_i$是Embedding层矩阵里的列向量，也被称为$w_i$的input vector。$U_j$是softmax层矩阵里的行向量，也被称为$w_i$的output vector。

Skip-gram模型的本质是计算输入word的input vector与目标word的output vector之间的余弦相似度，并进行`softmax归一化`。（余弦相似度的范围不本来就是0-1吗--是把每个单词的余弦相似度凡在一个向量的不同元素上，再进行归一化，得到每个单词的概率）

然而，直接对词典里的V个词计算相似度并归一化，显然是一件极其耗时的impossible mission。为此，Mikolov引入了两种优化算法：层次Softmax（Hierarchical Softmax）和负采样（Negative Sampling）。
思想都是把出现频率高的词拿出来多用。

>Softmax函数，或称归一化指数函数，是逻辑函数的一种推广。它能将一个含任意实数的K维的向量 的“压缩”到另一个K维实向量σ(z)中，使得每一个元素的范围都在（0，1）之间，并且所有元素的和为1

* Hierarchical Softmax（层次Softmax）
至此还没有结束，因为如果单单只是接一个softmax激活函数，计算量还是很大的，有多少词就会有多少维的权重矩阵，所以这里就提出层次Softmax(Hierarchical Softmax)，使用Huffman Tree来编码输出层的词典，相当于平铺到各个叶子节点上，瞬间把维度降低到了树的深度。这棵Tree把出现频率高的词放到靠近根节点的叶子节点处，每一次只要做二分类计算（每个结点只有是和否两种选择，只要用softmax计算是还是否，就能得到这个单词的概率，然后根据概率相乘就得到每个词的概率），计算路径上所有非叶子节点词向量的贡献即可。

>`哈夫曼树`(Huffman Tree)
参考材料：https://blog.csdn.net/google19890102/article/details/54848262
可以把一个序列构建成一棵huffman树（一种二叉树）（根据每个元素出现的次数）
再根据这个树的排列方式，记录每个元素的编码，就作为huffman 编码
给定N个权值作为N个叶子结点，构造一棵二叉树，若该树的带权路径长度达到最小，称这样的二叉树为最优二叉树，也称为哈夫曼树(Huffman Tree)。哈夫曼树是带权路径长度最短的树，权值较大的结点离根较近。



* 负采样（Negative Sampling）
这种优化方式做的事情是，在正确单词以外的负样本中进行采样，最终目的是为了减少负样本的数量，达到减少计算量效果。将词典中的每一个词对应一条线段，所有词组成了[0，1］间的剖分，如下图所示，然后每次随机生成一个[1, M-1]间的整数，看落在哪个词对应的剖分上就选择哪个词，最后会得到一个负样本集合。
![负采样](image-34.png)

* 下采样（subsampling）
其基本思想是在训练时依概率随机丢弃掉那些高频的词
$$p_{discard}(w) = 1- \sqrt(\frac{t}{f(w)})$$
其中t是一个先验参数，一般取为$10^{-5}$，$f(w)$是$w$在语料中出现的频率
实验证明，这种下采样技术可以显著提高低词频的词向量的准确度




##### 连续词袋模型 CBoW 
continuous bag of words

连续词袋模型与跳词模型恰好相反，CBOW获得中间词两边的的上下文，然后用周围的词去预测中间的词，把中间词当做y，把窗口中的其它词当做x输入，x输入是经过one-hot编码过的，然后通过一个隐层进行求和操作，最后通过激活函数softmax，可以计算出每个单词的生成概率，接下来的任务就是训练神经网络的权重，使得语料库中所有单词的整体生成概率最大化，而求得的权重矩阵就是文本表示词向量的结果。

从数学上看，CBoW等价于一个词袋模型的向量乘以一个`embedding矩阵`



![CBoW](image-32.png)
* 与NNLM的异同：
  1. 移除前向反馈神经网络中非线性的hidden layer，直接将中间层的Embedding layer与输出层的softmax layer连接；
  2. 忽略上下文环境的序列信息：输入的所有词向量均汇总到同一个Embedding layer；
  3. 将Future words纳入上下文环境

>`Softmax回归`（Softmax Regression）又被称作多项逻辑回归（multinomial logistic regression），它是逻辑回归在处理多类别任务上的推广。
参考材料：https://www.52nlp.cn/fasttext


* 评价
对每个local context window单独训练，没有利用包 含在global co-currence矩阵中的统计信息
对多义词无法很好的表示和处理，因为使用了唯一的词向量


* Item2Vec
与word2vec有关的应用

本质上，word2vec模型是在word-context的co-occurrence矩阵基础上建立起来的。因此，任何基于co-occurrence矩阵的算法模型，都可以套用word2vec算法的思路加以改进。

比如，推荐系统领域的协同过滤算法。

协同过滤算法是建立在一个user-item的co-occurrence矩阵的基础上，通过行向量或列向量的相似性进行推荐。如果我们将同一个user购买的item视为一个context，就可以建立一个item-context的矩阵。进一步的，可以在这个矩阵上借鉴CBoW模型或Skip-gram模型计算出item的向量表达，在更高阶上计算item间的相似度。

关于word2vec更多应用的介绍，可以进一步参考这篇文献[10]。



##### fasttext
参考资料：https://github.com/NLP-LOVE/ML-NLP/tree/master/NLP/16.2%20fastText
这里面有好多好东西还有代码！！说得通俗易懂地！
代码：C:\Users\m00842394\Desktop\vscodelearning\用fasttext做新闻文本分类

cbow的优化，用子嵌入将构词信息引入cbow.

英语单词通常有其内部结构和形成⽅式。例如，我们可以从“dog”“dogs”和“dogcatcher”的字⾯上推测它们的关系。这些词都有同⼀个词根“dog”，但使⽤不同的后缀来改变词的含义。而且，这个关联可以推⼴⾄其他词汇。

* 和CBoW的关系
  
   在word2vec中，我们并没有直接利⽤构词学中的信息。⽆论是在跳字模型还是连续词袋模型中，我们都将形态不同的单词⽤不同的向量来表⽰。例如，“dog”和“dogs”分别⽤两个不同的向量表⽰，而模型中并未直接表达这两个向量之间的关系。鉴于此，fastText提出了`子词嵌⼊`(subword embedding)的⽅法，从而试图将构词信息引⼊word2vec中的CBOW。

   和CBOW一样，fastText模型也只有三层：输入层、隐含层、输出层（Hierarchical Softmax），输入都是多个经向量表示的单词，输出都是一个特定的target，隐含层都是对多个词向量的叠加平均。

   不同的是，

   CBOW的输入是目标单词的上下文，fastText的输入是多个单词及其n-gram特征，这些特征用来表示单个文档；
   CBOW的输入单词被one-hot编码过，fastText的输入特征是被embedding过；
   CBOW的输出是目标词汇，==fastText的输出是文档对应的类标==。

* 核心思想
  将整篇文档的词及n-gram向量叠加平均得到文档向量，然后使用文档向量做softmax多分类


##### sense2vec
word2vec模型的问题在于词语的多义性。比如duck这个单词常见的含义有水禽或者下蹲，但对于 word2vec 模型来说，它倾向于将所有概念做归一化平滑处理，得到一个最终的表现形式。



### 训练模型
 得到没那么稀疏的矩阵，使用机器学习模型，比如朴素贝叶斯，因为它是适用于分类的问题的算法嘛。这种算法最适合这种大型数据集了，因为它会独立考察每个特征，计算每个类别的概率，然后预测概率最高的类别。



#### 用`词嵌入矩阵`构建神经网络
词向量可以作为神经网络的权重。
具体是这样的:
![我喜欢这篇文章](image-23.png)
* 首先，将语料转化为单词id的填充(padded)序列，得到一个特征矩阵。I like this article ---> \[10,55,700,3,0,0,0,0,0,0,0,0,0,0,0](len:15)
* 然后，创建一个`嵌入矩阵`(embedding matrix) ，使id为N的词`嵌入向量`位于第N行。
   >这个嵌入矩阵就是我们构建好的
   emdedding层是模型的核心部分，它一般在整个网络第一层。
   实现：在Pytorch中有专门的nn.embedding层来实现该部分，也可以自己构建embedding层，可以一层，也可以多层。训练结束后，把输入到embedding，embedding的输出为该单词嵌入表示
   
   如何理解`嵌入矩阵`：
   >* `单词索引`：每个单词在词汇表（vocabulary）中都有一个唯一的索引（index）。
   >* `嵌入维度`：嵌入矩阵中的每个向量都有固定的维度（例如，100维、300维等），这个维度是超参数，可以根据任务和数据集的大小进行调整。
   >* `嵌入矩阵`：是一个二维数组（或矩阵），其行数与词汇表的大小相同，列数与嵌入向量的维度相同。矩阵的每一行都是一个单词的`嵌入向量`。

* 最后，建立一个带有嵌入层的神经网络，对序列中的每一个词都用相应的向量进行加权

#### 构建神经网络
* 一个嵌入层，如前文所述, 将文本序列作为输入, 词向量作为权重。
* 一个简单的Attention层，它不会影响预测，但它可以捕捉每个样本的权重, 以便将作为一个不错的解释器（对于预测来说它不是必需的，只是为了提供可解释性，所以其实可以不用加它）。这篇论文（2014）提出了序列模型（比如`LSTM`）的Attention机制，探究了长文本中哪些部分实际相关。
* `两层双向LSTM`，用来建模序列中词的两个方向。
* 最后两层全连接层，可以预测每个新闻类别的概率。

#### 模型训练及评估
在训练集上划一小块验证集来验证模型性能
评估类似前面的

#### 解释
因为在神经网络中放了一个Attention层来提取每个词的权重，我们可以了解这些权重对一个样本的分类贡献有多大。所以这里我将尝试使用Attention权重来构建一个解释器（类似于上一节里的那个）

#### 总结
大概理解了，但是具体实现不太懂
代码还没撸
怎么把padding序列和嵌入矩阵结合的？
因为padding的编码其实是没有意义的，所以怎么把文本序列作为输入，词向量作为权重，是把这个编码166和166对应的向量[1,244,2,33,44,4,433,2...]加权求和吗？感觉没有意义，肯定不是相乘，这个权重是一个矩阵，神经网络自己会学习到吗？什么是神经网络呢，具体实现是咋样？
lstm是什么，序列模型
感觉看完代码可能就懂了。好慢慢。慢慢来。。。


#### 语言模型bert
语言模型, 即上下文/动态词嵌入（Contextualized/Dynamic Word Embeddings），克服了经典词嵌入方法的最大局限：多义词消歧义，一个具有不同含义的词（如" bank "或" stick"）只需一个向量就能识别。最早流行的是 ELMO（2018），它并没有采用固定的嵌入，而是利用双向 LSTM观察整个句子，然后给每个词分配一个嵌入。

到Transformers时代, 谷歌的论文Attention is All You Need（2017）提出的一种新的语言建模技术，在该论文中，证明了序列模型（如LSTM）可以完全被Attention机制取代，甚至获得更好的性能。

而后谷歌的BERT（Bidirectional Encoder Representations from Transformers，2018）包含了ELMO的上下文嵌入和几个Transformers，而且它是双向的（这是对Transformers的一大创新改进）。BERT分配给一个词的向量是整个句子的函数，因此，一个词可以根据上下文不同而有不同的词向量。

##### 对预训练模型进行精调用于文本分类
我打算对预训练模型进行精调(`迁移学习`)，从预训练的轻量 BERT 中进行迁移学习，人称 Distil-BERT  （用6600 万个参数替代1.1 亿个参数）
>为了完成文本分类任务，可以用3种不同的方式来使用BERT:
从零训练它，并将其作为分类器使用。
提取词嵌入，并在嵌入层中使用它们（就像上面用Word2Vec那样）。
对预训练模型进行精调(`迁移学习`)。

### 用Lime解释结果`lime包`


## 示例2
来自文心一言，基于自然语言处理的文本分类过程
针对您描述的基于自然语言处理的分类问题，您可以使用多种机器学习或深度学习模型来实现文字描述到标签（A、B、C、D）的分类。以下是一些常见的步骤和推荐的模型方法：

### 数据预处理
文本清洗：去除文本中的无用信息，如标点符号、数字、停用词等。spart描述中有中文、英文、字符$（）-‘ ’ *$，数字

### 分词
参考材料：https://github.com/NLP-LOVE/ML-NLP/tree/master/NLP/16.1%20Word%20Embedding

分词/词形还原/词干提取：将文本分割成有意义的单元（词或词组），并进行词形还原或词干提取以减少词汇的多样性。

1. 分词方法：
基于词表的分词方法
正向最大匹配法(forward maximum matching method, FMM)
逆向最大匹配法(backward maximum matching method, BMM)
N-最短路径方法
基于统计模型的分词方法
基于N-gram语言模型的分词方法
基于序列标注的分词方法
基于HMM的分词方法
基于CRF的分词方法
基于词感知机的分词方法
基于深度学习的端到端的分词方法

2. 工程方案：
从工程角度来看，目前分词已经有了十分成熟工程实现了，如IK，ansj等，列出一些比较常用的中文分词方案，以供大家学习使用：
结巴分词 `jieba.cut(s,True)`分词
ansj分词器
中科院计算所NLPIR 
哈工大的LTP 
清华大学THULAC 
斯坦福分词器 (Github)
Hanlp分词器 
KCWS分词器




### 特征工程
将文本转换为数值特征，以便机器学习模型可以处理
* 词袋模型（Bag of Words, BoW）：统计每个词在文档中出现的频率。
* TF-IDF：Term Frequency - Inverse Document Frequency 的缩写。考虑词的频率及其重要性（逆文档频率），以更好地表示文本。
>TF-IDF 可以用于评估一个字词x在语料中的一篇文档y中的重要程度，基本思想是:如果某个字词在一篇文档中出现的频率较高，而在其他文档中出现频率较低，则认为这个字词更能够代表这篇文档。
$$
w_{x,y} = TF_{x,y}*IDF_x
$$
其中，
$$
TF_x = \frac{在某一文章y中词条x出现的次数}{该文章中所有词条的数目}
$$
$$
IDF = log(\frac{语料库的文档总数}{包含词条x的文档总数+1})
$$
词嵌入（Word Embeddings）：如Word2Vec, GloVe, 或BERT的embeddings，将词转换为高维向量，以捕捉语义信息。

* `BM25` 算法的全称为 Okapi BM25，是一种搜索引擎用于评估查询和文档之间相关程度的排序算法，其中 BM 是 Best Match 的缩写

### 计算文本相似度
https://cloud.tencent.com/developer/article/1389446
内里有java代码可通过大模型转成python

#### 距离
1. 欧式距离
2. 曼哈顿距离：|x1-x2|+|y1-y2|
3. 切比雪夫距离：max(|x1-x2|, |y1-y2|)
4. 余弦距离：$cos\theta = \frac{a^2+b^2-c^2}{2ab}$
含义：两个空间向量的夹角，当b是1 的时候
应用：判断评分的相似度（类似于归一化比较，偏好相似度），比如一个人对两道菜的评分是2,3，另一个人对两道菜的评分是4,6，他们的余弦距离(2,3),(4,6)就是1，那么他们对这两道菜的偏好是一样的。
5. 汉明距离
在信息论中，表示为两个「等长」字符串之间对应位置的不同字符的个数。换句话说，汉明距离就是将一个字符串变换成另外一个字符串所需要「替换」的字符个数。如下：
0110与1110之间的汉明距离是1；
0100与1001之间的汉明距离是3；


#### 文本相似度的方法
1. 算法
基于词向量（欧几里德距离、曼哈顿距离和余弦距离）
基于具体字符（汉明距离）
基于概率统计
基于词嵌入的
2. 打开余弦相似度算法
分词，
把词取并集，
统计每个样本在并集上的频次，收成向量，
计算两两向量的余弦距离。
优点：简单直接，缺点：时间复杂度O($n^2$),没有考虑语序，不同语境下同一词的含义。这个可以直接用于打标签了。
3. 打开simhash
简单来说，simhash中使用了一种局部敏感型的hash算法。所谓局部敏感性hash，与传统hash算法不同的是（如MD5，当原始文本越是相似，其hash数值差异越大），simhash中的hash对于越是相似的内容产生的签名越相近。
(1)
第一步，分词
对文本进行分词操作，同时需要我们同时返回当前词组在文本内容中的权重（这基本上是目前所有分词工具都支持的功能）。
(2)
第二步，计算hash
`对于每一个得到的词组做hash`（就是一个编码过程，hashlib库提供了多种安全场景下的加密哈希算法的实现，如SHA-1、SHA-256、MD5等。一般的场景可以用hash()函数），
将词语表示为到01表示的bit位，需要保证每个hash结果的位数相同，如图中所示，使用的是8bit。
也可以不用每个词，选高频的top10,top50等都可以，这个选择会影响我们后面的结果，选得少比选得多更可能相似。
(3)
第三步，加权
根据每个词组对应的权重，对hash值做加权计算（bit为1则取为1做乘积，bit为0则取为-1做乘积），如上图中，
10011111与权重2加权得到[2 -2 -2 2 2 2 2 2]；
01001011与权重1加权得到[-1 1 -1 -1 1 -1 1 1]；
01001011与权重4加权后得到[-4 4 -4 -4 4 -4 4 4]；
(4)
第四步，纵向相加
将上述得到的加权向量结果，进行纵向相加实现降维，如上述所示，得到[-3 3 -7 -3 7 -3 7 7]
(5)
第五步，归一化
将最终降维向量，对于每一位大于0则取为1，否则取为0，这样就能得到最终的simhash的指纹签名[0 1 0 0 1 0 1 1]
(6)
第六步，相似度比较：
通过上面的步骤，我们可以利用SimHash算法为每一个网页生成一个向量指纹，在simhash中，判断2篇文本的相似性使用的是海明距离。什么是汉明距离？前文已经介绍过了。
(7)
判断：
   1. 在经验数据上，我们多认为两个文本的汉明距离<=3的话则认定是相似的。
   2. 另外一点需要需要注意的是，simhash的优点是适用于高维度的海量数据处理，当维度降低，如短文本的相似度比较，simhash并不合适，因为本身词就很小，还把他们全分词算hash了，即使有一点偏差都会显示出来。
   3. 选择高频top n的n对结果影响很大。


### 选择模型
1. 传统机器学习模型：
逻辑回归：适用于特征空间较小且线性可分的情况。
朴素贝叶斯：在文本分类中表现良好，特别是当数据稀疏时。
支持向量机（SVM）：在高维空间中表现优异，但计算成本可能较高。
决策树/随机森林：能够处理非线性关系，且不需要太多的特征工程。
2. 深度学习模型：
卷积神经网络（CNN）结合词嵌入：用于捕捉文本的局部特征。解决视觉问题。
循环神经网络（RNN）及其变体（如LSTM, GRU）：擅长处理序列数据，能够捕捉文本中的长期依赖。理解文本。任何能够理解语言的模型都必须捕捉词序，而循环神经网络是通过在一个序列中，一次处理一个单词来做到的。所以不能并行。
Transformer及其变体（如BERT, RoBERTa, GPT等）：基于自注意力机制，是目前最先进的模型之一，特别是在大规模预训练数据上表现卓越。Transformer可以非常有效地并行化。
   1. 模型训练与评估
训练：使用训练集数据训练模型。
验证：使用验证集（如果有的话）调整模型参数，防止过拟合。
评估：使用测试集评估模型性能，常见的评估指标包括准确率、精确率、召回率、F1分数等。
   1. 部署与应用
将训练好的模型部署到生产环境，对新的文本描述进行分类。
   1. 示例代码（使用BERT）
由于BERT等Transformer模型在NLP任务中的优异表现，这里给出一个基于BERT的示例思路（使用Python和Hugging Face的Transformers库）：

from transformers import BertTokenizer, BertForSequenceClassification 
from transformers import Trainer, TrainingArguments 

`# 初始化分词器和模型  `
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')  
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)  

`# 假设您已经有了训练数据和测试数据的编码  `
`# train_encodings = tokenizer(train_texts, truncation=True, padding=True)`  
`# test_encodings = tokenizer(test_texts, truncation=True, padding=True)  `
  
`# 定义训练参数  `
training_args = TrainingArguments(  
    output_dir='./results',          # 输出文件夹  
    num_train_epochs=3,              # 训练轮次  
    per_device_train_batch_size=16,  # 每个设备的批次大小  
    per_device_eval_batch_size=64,   # 评估批次大小  
    warmup_steps=500,                # 预热步数  
    weight_decay=0.01,               # 权重衰减  
    logging_dir='./logs',            # 日志记录文件夹  
    logging_steps=10,  
)  
  
`# 训练模型  `
trainer = Trainer(  
    model=model,                     # 模型  
    args=training_args,              # 训练参数  
    train_dataset=train_dataset,      # 训练数据集  
    eval_dataset=test_dataset         # 评估数据集  
)  
  
trainer.train()
注意：上述代码是一个简化的示例，实际使用时需要您根据自己的数据集来准备train_texts、test_texts，以及将文本数据转换为模型可接受的格式（例如，使用tokenizer函数处理文本并转换为编码）。同时，您可能还需要根据任务需求调整模型的参数和训练参数。




途径：开源社区hugging face 
这种社区是怎么用的呀，
感觉那些人不用别人带也能摸上这些网站用起来，
我也试试

# python框架
2024最详细的AI框架对比指南—PyTorch与TensorFlow到底选谁？:
https://cloud.tencent.com/developer/article/2389961
mindspore（华为自己的）
我还用过django，广泛使用的全堆栈 Python Web 框架，用于开发大型 Web 应用程序
Python 框架是程序包和模块的合辑。
>模块是一组相关代码，程序包是一组模块。开发人员可以使用 Python 框架快速构建 Python 应用程序而不必担心低级别详细信息。

## PyTorch
PyTorch 是一个建立在 Torch 库之上的 ML 框架，其是另一个开源 ML 库。  开发人员将其用于 NLP、机器人技术和计算机视觉等应用程序，以及发掘图像和视频中有意义的信息。他们也将其用于在 CPU 和 GPU 中运行这些应用程序。
PyTorch 的基本组件：
张量（Tensors）：PyTorch 中的基本数据结构，类似于 NumPy 的 ndarray，但可以在 GPU 上运行以加速计算。
自动微分（Autograd）：PyTorch 的自动微分系统可以自动计算张量上所有操作的梯度。这对于训练神经网络至关重要。
神经网络模块（nn.Module）：PyTorch 提供了 nn.Module，这是一个用于构建所有神经网络模型的基类。你可以通过继承这个类来定义自己的神经网络层或整个模型。
优化器（Optimizers）：PyTorch 提供了多种优化算法，如 SGD、Adam 等，用于更新神经网络的权重。
数据加载（Data Loading）：PyTorch 提供了 torch.utils.data.DataLoader，它可以让你轻松地加载和批处理数据。

## TensorFlow
TensorFlow的基本概念包括：
1.张量（Tensor）：存储和传递数据的多维数组，包括标量、向量、矩阵等。
2.计算图（Graph）：用于描述数据流的有向无坏图，图中节点表示操作，边表示数据流
3.会话（Session）：用于执行计算图中的操作，并将输出结果返回

TensorFlow的使用场景包括：
1.图像识别和处理：使用卷积神经网络训练模型进行图像分类、目标检测、图像分割等任务。
2.自然语言处理：使用循坏神经网络和长短时记忆网络等模型进行又本分类、情感分析、机器翻译等任务。
3.聚类和降维：使用自编码器等模型对数据进行聚类和降维，提取数据特征。
AutoEncoder作为NN里的一类模型，采用无监督学习的方式对高维数据进行高效的特征提取和特征表示，并且在学术界和工业界都大放异彩https://zhuanlan.zhihu.com/p/68903857
4.强化学习：使用强化学习算法构建智能体，并使用TensorFlow训练智能体模型。以下是一些例子，可以帮助新手更容易地理解和字习。

# Transformer
但是如果你想要真正的跟随潮流，并且你写Python，我强烈推荐由HuggingFace公司维护的Transformer库。该平台允许您以一种非常开发人员友好的方式训练和使用当今大多数流行的NLP模型，如BERT、Roberta、T5、GPT-2。
谷歌的Transformer模型（bert 用了）
Transformer是一种神经网络结构
BERT的基础集成单元是Transformer的Encoder
能够理解语言的模型
我们的打标签没有应用语序，只是捕捉关键词，所以分词--计算相似度--分类就可以。
Transformer背后的创新可以归结为三个主要概念：
![transformer](image-6.png)
![transformer2](image-18.png)

1. 位置编码（Positional Encodings）
   把依靠神经网络的顺序转到依靠数据结构上
2. 注意力机制（attention）
   注意力是一种神经网络结构，在机器学习中随处可见。注意力是一种机制，它允许文本模型在决定如何翻译输出句子中的单词时“查看”原始句子中的每一个单词。模型如何知道在每个时间步中应该“注意”哪些单词呢? 这就是从训练数据中学到的东西。通过观察成千上万的法语和英语句子，该模型学会了什么类型的单词是相互依赖的。学会了如何遵守词性、复数和其他语法规则。
3. 自注意力机制（Self-Attention）
## 原理步骤：
文字变成token
理解token（词元）
把token转化成文字
### 文字变token
![token变向量](image-7.png)
token的中文叫做词元，是一个基本的单元
tokenization：成功的关键愿原因-成功 的 关键 原 因
可以预训练去分，或者统计一下'原因'一起出现的概率有多大，单个出现的概率更高，就分开.
P(原因） = C（原因）/（C(原)+C(因))
如果是英文的话就用词根、词缀去分词
词表：一个词就有一个ID，用ID去替代中文的词
如果只用单纯的token id 就不能表示词和词之间的意思
所以要用embedding,把ID 转化成高维的向量（每个不同模型的id和向量都不同），没有明确的结论，到底在哪几个维度去定义这个token，这个不需要提前定义，是模型自己初始化的向量。所以你不知道有维度的个数是多少，只能设置需要多少参数（超参：提前定义好需要多少维度）
在各个维度意思相近的token有相近的向量

### 位置信息
每个位置有独特的向量，叫做位置向量(positional embedding)
可以是人工设计的，也可以是训练得到的（怎么训练得到呢？不就是按顺序吗）

### attention 理解上下文
![attention](image-8.png)
注意力机制：通过计算token之间的相关性，来理解每个token的语义
计算一个字和其他所有字的相关性，得到这个字的相关性的加权和

### add+normal
add: X -->X'+delta ,这个delta就是经过函数运算后丢了的信息，给他加回去
normal: 正则化，加完之后，把delta缩放到比如1-10的区间里，不让太大
因为transformer的层能叠到非常多，丢失太多肯定不行，加完之后又太多的加了，用normal去正则化一下。（目的是？

### multi-head attention 多注意力机制
![multi-head attention](image-9.png)
上面，单一地计算词与词的相关性，没有考虑在语境中词元的含义
在不同的维度计算相关性，然后通过feed-forward把每个维度分析的相关性之后整合思考了一下，变成一个一维的向量
![feed forward](image-15.png)
![反复思考](image-16.png)
### output layer
![output](image-17.png)
线性变化+software
向量到字之前：得到一个概率分布
每个词表上可能成为下一个token的概率，概率最大的，就生成这个词
beam search 一次性看好几个词，一起看，下一个是谁的概率最大，就选谁

## 为什么超长文本是挑战
![超长文本](image-10.png)
需要的用于计算的内存太大了：可能选的词太多，需要两两计算词元的相关性
稀疏矩阵：两个token 肯定没关系，用不到的词表就直接设0 了，就不算了。

## transformer的3种变体
![transformer变体](image-11.png)
encoder架构：bert只有编码器，擅长语义理解。
decoder架构：主流的GPT只有解码器，没有编码器，只用解码器的效果比较好
跟矩阵中的秩有关，如果有1万个向量，就一定会有1万个值，就是满秩的，信息量是足够的。
encoder-decoder架构：T5

# GPT 
generative pre-trained transformer
生成预训练的transformer
GPT在做什么-文字接龙
![GPT在做什么](image-12.png)
采样：随机性，随机选概率最大的，或者是概率第二的，是随机的，所以每次生成的文字不一定一样
需要构造非常多的语料，一段文字就可以看做是一个语料，代码和书籍都可以作为输入，无监督的，构造出来就完了。
最开始的时候输入某些字，语料的标准答案是工（这就相当于有标签），无数的预料都用这个方法训练之后，就能学进去。
## GPT-1
117M：函数的参数量（天生的资质）
1G： 用来学文字接龙的资料（后天的努力）
没法回答问题，能进行情感分类

## GPT-2
1542M
能力：简单的问答，摘要（神奇咒语，在后面加个tl,dr（too long, don't read），就能做简单摘要）

## GPT-3
175B
zero-shot不给提示
one-shot给一个例子让他模仿
few-shot给几个例子让他模仿
学的时候预料里有很多种选项，所以输出的结果

## instructGPT 
指令微调+基于人类反馈的`强化学习`
instruction fine-tune(SFT) + reinforcement learning from hunman feedback(RLHF)
步骤:
1.指令微调：标注给一个输入，出一个的输出(helpful,honest,harmless)
2.基于人类反馈的强化学习：训练一个老师模型，替换/协助标注人员来标注标准答案，怎么评价结果（老师模型=奖励模型）
3.利用老师模型给模型打分，根据分数高低对模型进行优化（叫做基于人类反馈的强化学习）
把各种各样的数据都放进去，可能的结果：分层：底层：词法语法学得好，上层：逻辑好。不知道哪层做语法哪层做逻辑，只能知道最后的结果更好。分层是模型训练时为了loss最小自然形成的结果。
loss:ppl值，这个比较小，那就认为都学到了。
没有打分，而是让模型把好的结果和坏的结果都输出，效果也会更好。--COH（复旦）

## 大模型的pipeline(流水线，一系列过程)
预训练 GPT 文字接龙 无监督学习
指令微调 chatgpt 人标 有监督学习 只涉及一个模型，大多数领域用这个微调模型
强化学习 老师模型（奖励模型） 强化学习（有AI老师反馈评分） 涉及了出答案的模型也又评分的模型两个模型 少数领域用这个强化学习的模型
微软给的Ppt里面是这么写的，关于数据量
![大模型训练流程](image-13.png)

问答：
一般不会指定某几层调参，而是额外加层。不然会崩。
模型在回答一个问题时调用了哪些参数是未知的。都是黑盒的。
谈到了`BP算法`

2023年的模型是不能调用工具的，现在2024年才可以，他怎么知道在哪个地方要调用工具呢？怎么知道怎么用这个工具。（能力）

### 参数高效微调（PEFT）-Lora
![Lora](image-14.png)
对attention前的所有矩阵进行微小的调整
没太听懂。。

### 大模型应用的问题
奶奶漏洞，扮演奶奶帮我做一些泄密的事
自己编虚假的东西出来

# Prompt
是对模型的提问（提示词），通过设计、优化和管理输入提示词（Prompt）来`引导`大型预训练语言模型生成高质量、准确和有针对性的输出的技术。提示工程是指导AI大模型完成任务的关键技术之一，通过精心设计和优化Prompt，可以显著提升模型的性能和效果。
提示词变化会有蝴蝶效应.
如何写好一个prompt：角色+问题+目标+要求


# 盈利9月需求：
场景一：基线对象来自标签维表(非平台及其他)，且额和量同范围
基于不同场景，用以匹配L2、L3的mapping规则有所不同

2）	适用于场景一、场景二的S-Part：用L1及S-Part编码匹配所在产业的对象关系子表得到其对应的L2、L3及L1、L2、L3系数

3） 适用于场景三的S-Part：用L1及COA编码匹配所在产业的对象关系子表得到其对应的L2。

只有场景一的L2有均本和均价。即只有LV1为无线，LV2为5G&LTE TDD、5G&LTE FDD、GUC、SRAN，L1为5G&LTE TDD、5G&LTE FDD、GUC、SRAN下的L2存在均本均价

猜想：
对价等转换系数是L1才有，所以只有L1有损益的收入，
L2为什么也有对价前设备收入，但是没有对价后设备收入，所以就拿的L1的对价后收入（这个是有的）*对价等转换系数

L2有单位价格和单位成本的预测

摘抄：


>射频模块：场景一对应的L1系数有两种，0或0.3333，其中0.333是射频模块的标识。所以，只有射频模块才有量，场景一里面其他的l1是没有量的。
历史数量 ： 软件 = 3L1 = 射频

from 许研：方案二：
1.分析师修改ran下的软件的L2系数
2. 重新计算软件的收入数量给知识表示 = l2系数*spart量（l2系数不一定是1，由分析师改）
历史发货量 = l2系数*spart量（l2系数不一定是1，由分析师改）
计划snop发货量 ，知识表示是直接用射频的量作为软件的计划量* 结转率---》软件的结转量
其他 有量吗？
其他还是沿用射频的量。


>场景一的l1：
场景一的l1一共有4种，分别是fdd,tdd,sran，guc,他们的l1系数都有0或者0.3333，但是，系数是0的就相当于L1里面不会包含这个spart的量了，也就相当于只有0.3333.
发货量sop = sum(spart/3)
发货量历史 = sum(l1系数\*spart发货量)
收入量历史 = sum(l1系数\*spart收入量)

>场景一,l2为软件、其他：
对应的l1只有那4个
l2的单位价格 = l2对价前成本金额/l2收入数量
（分母则用这个l2所在的l1射频模块量*3计算，因为l1系数是0.333，所以相当于分母是直接 spart发货量\*I(l1系数=0)相加 ）
（可以推出，场景一，l2为软件、其他的, l1都是射频模块,是的l1系数不是0的只有0.3333）
（这些l2，其他的发货量sop和发货量历史也是、*3这么算的分母）
其他的l2的数量:  直接sum的spart的量

收入占比= l2对价前收入/l1对价前收入
以下来自周慧：
场景2:
预测其他产业收入占比及L2非其他产业收入->计算L1产业收入（=sum(l2非其他产业占比)/（1-其他占比））->计算L2非其他产业收入占比（=l2收入/l1收入）
场景1：
预测L2产业收入->计算L1产业收入（=sum(l1收入)）->计算L2产业收入占比（=l2收入/l1收入）
![alt text](image-40.png)
![alt text](image-42.png)
![alt text](image-43.png)

ict损益没有spart,不用打l2

## 场景
场景一：所有Spart都计量；软件和其他的量使用射频模块的量替代，对所有L2的量×价求和即可得到L1对价前收入金额预测值。
场景二：部分Spart计量；不计量的归类为其他，对其他以外L2的量×价求和再除以（ 1-其他占比）得到L1对价前收入金额预测值。
场景三：所有Spart都不计量；直接对L1对价后收入金额进行时序预测。

ran 下面区分射频、基带、软件、其他（这些是量纲层级的）
## 预测因子
预测因子计算规则： 
LV1	设备收入	制毛	　	　	　
L1	设备收入	制毛率	MCA调整率	制毛调整率	结转率
L2	均本	均价	收入占比	制毛率	　
		L2层级：单价 = L2收入金额/L2收入数量；单本 = L2成本金额/L2收入数量；收入占比 = L2收入金额/L1收入金额。
		L1层级：结转率 = L1收入数量/L1发货数量 ；MCA调整率 = 1 – 对价后收入/对价前收入；制毛调整率 = 对价前成本/对价前收入 - 对价后成本/对价后收入。
		LV1层级：设备收入 = 收入金额；制毛率 = 1 - 成本金额/收入金额。

